<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>API Reference &mdash; groupyr 0.2.7 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/groupyr.css" type="text/css" />
      <link rel="stylesheet" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" type="text/css" />
      <link rel="stylesheet" href="_static/panels-bootstrap.5fd3999ee7762ccc51105388f4a9d115.css" type="text/css" />
      <link rel="stylesheet" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" type="text/css" />
      <link rel="stylesheet" href="_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="_static/sg_gallery-rendered-html.css" type="text/css" />
    <link rel="shortcut icon" href="_static/groupyr-logo.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/js/copybutton.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Frequently Asked Questions" href="faq.html" />
    <link rel="prev" title="Getting help using groupyr" href="getting_help.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> groupyr
            <img src="_static/groupyr-logo.svg" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                0.2.7
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="index.html">Home</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto_examples/index.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting_help.html">Getting help using <em>groupyr</em></a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">API Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#sparse-groups-lasso-estimators">Sparse Groups Lasso Estimators</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cross-validation-estimators">Cross-validation Estimators</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dataset-generation">Dataset Generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#regularization-paths">Regularization Paths</a></li>
<li class="toctree-l2"><a class="reference internal" href="#group-transformers">Group Transformers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing to <em>groupyr</em></a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/richford/groupyr">Groupyr on GitHub</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">groupyr</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>API Reference</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/api.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="api-reference">
<h1>API Reference<a class="headerlink" href="#api-reference" title="Permalink to this headline"></a></h1>
<p><em>Groupyr</em> contains estimator classes that are fully compliant
with the <a class="reference external" href="https://scikit-learn.org">scikit-learn</a> ecosystem. Consequently,
their initialization, <code class="docutils literal notranslate"><span class="pre">fit</span></code>, <code class="docutils literal notranslate"><span class="pre">predict</span></code>, <code class="docutils literal notranslate"><span class="pre">transform</span></code>, and <code class="docutils literal notranslate"><span class="pre">score</span></code>
methods will be familiar to <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> users.</p>
<section id="sparse-groups-lasso-estimators">
<h2>Sparse Groups Lasso Estimators<a class="headerlink" href="#sparse-groups-lasso-estimators" title="Permalink to this headline"></a></h2>
<p>These are <em>groupyr</em>’s canonical estimators. <code class="docutils literal notranslate"><span class="pre">SGL</span></code> is intended for regression
problems while <code class="docutils literal notranslate"><span class="pre">LogisticSGL</span></code> is intended for classification problems.</p>
<dl class="py class">
<dt class="sig sig-object py" id="groupyr.SGL">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">groupyr.</span></span><span class="sig-name descname"><span class="pre">SGL</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">l1_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_l2_by</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'group_length'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-07</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">suppress_solver_warnings</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_solver_trace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/groupyr/sgl.html#SGL"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#groupyr.SGL" title="Permalink to this definition"></a></dt>
<dd><p>An sklearn compatible sparse group lasso regressor.</p>
<p>This solves the sparse group lasso <a class="reference internal" href="#rcbd543aab8f7-1" id="id1">[1]</a> problem for a feature matrix
partitioned into groups using the proximal gradient descent (PGD)
algorithm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>l1_ratio</strong><span class="classifier">float, default=1.0</span></dt><dd><p>Hyper-parameter : Combination between group lasso and lasso. l1_ratio=0
gives the group lasso and l1_ratio=1 gives the lasso.</p>
</dd>
<dt><strong>alpha</strong><span class="classifier">float, default=1.0</span></dt><dd><p>Hyper-parameter : overall regularization strength.</p>
</dd>
<dt><strong>groups</strong><span class="classifier">list of numpy.ndarray</span></dt><dd><p>list of arrays of non-overlapping indices for each group. For
example, if nine features are grouped into equal contiguous groups of
three, then groups would be <code class="docutils literal notranslate"><span class="pre">[array([0,</span> <span class="pre">1,</span> <span class="pre">2]),</span> <span class="pre">array([3,</span> <span class="pre">4,</span> <span class="pre">5]),</span>
<span class="pre">array([6,</span> <span class="pre">7,</span> <span class="pre">8])]</span></code>. If the feature matrix contains a bias or
intercept feature, do not include it as a group. If None, all
features will belong to one group. We set groups in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> so
that it can be reused in model selection and CV routines.</p>
</dd>
<dt><strong>scale_l2_by</strong><span class="classifier">[“group_length”, None], default=”group_length”</span></dt><dd><p>Scaling technique for the group-wise L2 penalty.
By default, <code class="docutils literal notranslate"><span class="pre">scale_l2_by=&quot;group_length</span></code> and the L2 penalty is
scaled by the square root of the group length so that each variable
has the same effect on the penalty. This may not be appropriate for
one-hot encoded features and <code class="docutils literal notranslate"><span class="pre">scale_l2_by=None</span></code> would be more
appropriate for that case. <code class="docutils literal notranslate"><span class="pre">scale_l2_by=None</span></code> will also reproduce
ElasticNet results when all features belong to one group.</p>
</dd>
<dt><strong>fit_intercept</strong><span class="classifier">bool, default=True</span></dt><dd><p>Specifies if a constant (a.k.a. bias or intercept) should be
added to the linear predictor (X &#64; coef + intercept).</p>
</dd>
<dt><strong>max_iter</strong><span class="classifier">int, default=1000</span></dt><dd><p>Maximum number of iterations for PGD solver.</p>
</dd>
<dt><strong>tol</strong><span class="classifier">float, default=1e-7</span></dt><dd><p>Stopping criterion. Convergence tolerance for the <code class="docutils literal notranslate"><span class="pre">copt</span></code> proximal gradient solver</p>
</dd>
<dt><strong>warm_start</strong><span class="classifier">bool, default=False</span></dt><dd><p>If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, reuse the solution of the previous call to <code class="docutils literal notranslate"><span class="pre">fit</span></code>
as initialization for <code class="docutils literal notranslate"><span class="pre">coef_</span></code> and <code class="docutils literal notranslate"><span class="pre">intercept_</span></code>.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int, default=0</span></dt><dd><p>Verbosity flag for PGD solver. Any positive integer will produce
verbose output</p>
</dd>
<dt><strong>suppress_solver_warnings</strong><span class="classifier">bool, default=True</span></dt><dd><p>If True, suppress convergence warnings from PGD solver.
This is useful for hyperparameter tuning when some combinations
of hyperparameters may not converge.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="rcbd543aab8f7-1"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Noah Simon, Jerome Friedman, Trevor Hastie &amp; Robert Tibshirani,
“A Sparse-Group Lasso,” Journal of Computational and Graphical
Statistics, vol. 22:2, pp. 231-245, 2012
DOI: 10.1080/10618600.2012.681250</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>coef_</strong><span class="classifier">array of shape (n_features,)</span></dt><dd><p>Estimated coefficients for the linear predictor (<cite>X &#64; coef_ +
intercept_</cite>).</p>
</dd>
<dt><strong>intercept_</strong><span class="classifier">float</span></dt><dd><p>Intercept (a.k.a. bias) added to linear predictor.</p>
</dd>
<dt><strong>n_iter_</strong><span class="classifier">int</span></dt><dd><p>Actual number of iterations used in the solver.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="groupyr.LogisticSGL">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">groupyr.</span></span><span class="sig-name descname"><span class="pre">LogisticSGL</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">l1_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_l2_by</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'group_length'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-07</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">suppress_solver_warnings</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_solver_trace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/groupyr/logistic.html#LogisticSGL"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#groupyr.LogisticSGL" title="Permalink to this definition"></a></dt>
<dd><p>An sklearn compatible sparse group lasso classifier.</p>
<p>This solves the sparse group lasso <a class="reference internal" href="#rd799dd0ca6ec-1" id="id3">[1]</a> problem for a feature matrix
partitioned into groups using the proximal gradient descent (PGD)
algorithm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>l1_ratio</strong><span class="classifier">float, default=1.0</span></dt><dd><p>Hyper-parameter : Combination between group lasso and lasso. l1_ratio=0
gives the group lasso and l1_ratio=1 gives the lasso.</p>
</dd>
<dt><strong>alpha</strong><span class="classifier">float, default=0.0</span></dt><dd><p>Hyper-parameter : overall regularization strength.</p>
</dd>
<dt><strong>groups</strong><span class="classifier">list of numpy.ndarray</span></dt><dd><p>list of arrays of non-overlapping indices for each group. For
example, if nine features are grouped into equal contiguous groups of
three, then groups would be <code class="docutils literal notranslate"><span class="pre">[array([0,</span> <span class="pre">1,</span> <span class="pre">2]),</span> <span class="pre">array([3,</span> <span class="pre">4,</span> <span class="pre">5]),</span>
<span class="pre">array([6,</span> <span class="pre">7,</span> <span class="pre">8])]</span></code>. If the feature matrix contains a bias or
intercept feature, do not include it as a group. If None, all
features will belong to one group. We set groups in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> so
that it can be reused in model selection and CV routines.</p>
</dd>
<dt><strong>scale_l2_by</strong><span class="classifier">[“group_length”, None], default=”group_length”</span></dt><dd><p>Scaling technique for the group-wise L2 penalty.
By default, <code class="docutils literal notranslate"><span class="pre">scale_l2_by=&quot;group_length</span></code> and the L2 penalty is
scaled by the square root of the group length so that each variable
has the same effect on the penalty. This may not be appropriate for
one-hot encoded features and <code class="docutils literal notranslate"><span class="pre">scale_l2_by=None</span></code> would be more
appropriate for that case. <code class="docutils literal notranslate"><span class="pre">scale_l2_by=None</span></code> will also reproduce
ElasticNet results when all features belong to one group.</p>
</dd>
<dt><strong>fit_intercept</strong><span class="classifier">bool, default=True</span></dt><dd><p>Specifies if a constant (a.k.a. bias or intercept) should be
added to the linear predictor (X &#64; coef + intercept).</p>
</dd>
<dt><strong>max_iter</strong><span class="classifier">int, default=1000</span></dt><dd><p>Maximum number of iterations for PGD solver.</p>
</dd>
<dt><strong>tol</strong><span class="classifier">float, default=1e-7</span></dt><dd><p>Stopping criterion. Convergence tolerance for the <code class="docutils literal notranslate"><span class="pre">copt</span></code> proximal gradient solver</p>
</dd>
<dt><strong>warm_start</strong><span class="classifier">bool, default=False</span></dt><dd><p>If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, reuse the solution of the previous call to <code class="docutils literal notranslate"><span class="pre">fit</span></code>
as initialization for <code class="docutils literal notranslate"><span class="pre">coef_</span></code> and <code class="docutils literal notranslate"><span class="pre">intercept_</span></code>.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int, default=0</span></dt><dd><p>Verbosity flag for PGD solver. Any positive integer will produce
verbose output</p>
</dd>
<dt><strong>suppress_solver_warnings</strong><span class="classifier">bool, default=True</span></dt><dd><p>If True, suppress convergence warnings from PGD solver.
This is useful for hyperparameter tuning when some combinations
of hyperparameters may not converge.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="rd799dd0ca6ec-1"><span class="brackets"><a class="fn-backref" href="#id3">1</a></span></dt>
<dd><p>Noah Simon, Jerome Friedman, Trevor Hastie &amp; Robert Tibshirani,
“A Sparse-Group Lasso,” Journal of Computational and Graphical
Statistics, vol. 22:2, pp. 231-245, 2012
DOI: 10.1080/10618600.2012.681250</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>classes_</strong><span class="classifier">ndarray of shape (n_classes, )</span></dt><dd><p>A list of class labels known to the classifier.</p>
</dd>
<dt><strong>coef_</strong><span class="classifier">array of shape (n_features,)</span></dt><dd><p>Estimated coefficients for the linear predictor (<cite>X &#64; coef_ +
intercept_</cite>).</p>
</dd>
<dt><strong>intercept_</strong><span class="classifier">float</span></dt><dd><p>Intercept (a.k.a. bias) added to linear predictor.</p>
</dd>
<dt><strong>n_iter_</strong><span class="classifier">int</span></dt><dd><p>Actual number of iterations used in the solver.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</section>
<section id="cross-validation-estimators">
<h2>Cross-validation Estimators<a class="headerlink" href="#cross-validation-estimators" title="Permalink to this headline"></a></h2>
<p>These estimators have built-in <a class="reference external" href="https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-evaluating-estimator-performance">cross-validation</a>
capabilities to find the best values of the hyperparameters <code class="docutils literal notranslate"><span class="pre">alpha</span></code> and
<code class="docutils literal notranslate"><span class="pre">l1_ratio</span></code>. These are more efficient than using the canonical estimators
with grid search because they make use of warm-starting. Alternatively, you
can specify <code class="docutils literal notranslate"><span class="pre">tuning_strategy</span> <span class="pre">=</span> <span class="pre">&quot;bayes&quot;</span></code> to use <a class="reference external" href="https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html">Bayesian optimization over
the hyperparameters</a>
instead of a grid search.</p>
<dl class="py class">
<dt class="sig sig-object py" id="groupyr.SGLCV">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">groupyr.</span></span><span class="sig-name descname"><span class="pre">SGLCV</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">l1_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_l2_by</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'group_length'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-07</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tuning_strategy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'grid'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_bayes_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_bayes_points</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">suppress_solver_warnings</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/groupyr/sgl.html#SGLCV"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#groupyr.SGLCV" title="Permalink to this definition"></a></dt>
<dd><p>Iterative SGL model fitting along a regularization path.</p>
<p>See the scikit-learn glossary entry for <a class="reference external" href="https://scikit-learn.org/stable/glossary.html#term-cross-validation-estimator">cross-validation estimator</a></p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>l1_ratio</strong><span class="classifier">float or list of float, default=1.0</span></dt><dd><p>float between 0 and 1 passed to SGL (scaling between group lasso and
lasso penalties). For <code class="docutils literal notranslate"><span class="pre">l1_ratio</span> <span class="pre">=</span> <span class="pre">0</span></code> the penalty is the group lasso
penalty. For <code class="docutils literal notranslate"><span class="pre">l1_ratio</span> <span class="pre">=</span> <span class="pre">1</span></code> it is the lasso penalty. For <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;</span>
<span class="pre">l1_ratio</span> <span class="pre">&lt;</span> <span class="pre">1</span></code>, the penalty is a combination of group lasso and
lasso. This parameter can be a list, in which case the different
values are tested by cross-validation and the one giving the best
prediction score is used. Note that a good choice of list of values
will depend on the problem. For problems where we expect strong
overall sparsity and would like to encourage grouping, put more
values close to 1 (i.e. Lasso). In contrast, if we expect strong
group-wise sparsity, but only mild sparsity within groups, put more
values close to 0 (i.e. group lasso).</p>
</dd>
<dt><strong>groups</strong><span class="classifier">list of numpy.ndarray</span></dt><dd><p>list of arrays of non-overlapping indices for each group. For
example, if nine features are grouped into equal contiguous groups of
three, then groups would be <code class="docutils literal notranslate"><span class="pre">[array([0,</span> <span class="pre">1,</span> <span class="pre">2]),</span> <span class="pre">array([3,</span> <span class="pre">4,</span> <span class="pre">5]),</span>
<span class="pre">array([6,</span> <span class="pre">7,</span> <span class="pre">8])]</span></code>. If the feature matrix contains a bias or
intercept feature, do not include it as a group. If None, all
features will belong to one group. We set groups in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> so
that it can be reused in model selection and CV routines.</p>
</dd>
<dt><strong>scale_l2_by</strong><span class="classifier">[“group_length”, None], default=”group_length”</span></dt><dd><p>Scaling technique for the group-wise L2 penalty.
By default, <code class="docutils literal notranslate"><span class="pre">scale_l2_by=&quot;group_length</span></code> and the L2 penalty is
scaled by the square root of the group length so that each variable
has the same effect on the penalty. This may not be appropriate for
one-hot encoded features and <code class="docutils literal notranslate"><span class="pre">scale_l2_by=None</span></code> would be more
appropriate for that case. <code class="docutils literal notranslate"><span class="pre">scale_l2_by=None</span></code> will also reproduce
ElasticNet results when all features belong to one group.</p>
</dd>
<dt><strong>eps</strong><span class="classifier">float, default=1e-3</span></dt><dd><p>Length of the path. <code class="docutils literal notranslate"><span class="pre">eps=1e-3</span></code> means that
<code class="docutils literal notranslate"><span class="pre">alpha_min</span> <span class="pre">/</span> <span class="pre">alpha_max</span> <span class="pre">=</span> <span class="pre">1e-3</span></code>.</p>
</dd>
<dt><strong>n_alphas</strong><span class="classifier">int, default=100</span></dt><dd><p>Number of alphas along the regularization path, used for each l1_ratio.</p>
</dd>
<dt><strong>alphas</strong><span class="classifier">ndarray, default=None</span></dt><dd><p>List of alphas where to compute the models.
If None alphas are set automatically</p>
</dd>
<dt><strong>fit_intercept</strong><span class="classifier">bool, default=True</span></dt><dd><p>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(i.e. data is expected to be centered).</p>
</dd>
<dt><strong>normalize</strong><span class="classifier">bool, default=False</span></dt><dd><p>This parameter is ignored when <code class="docutils literal notranslate"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler" title="(in scikit-learn v1.1)"><code class="docutils literal notranslate"><span class="pre">sklearn.preprocessing.StandardScaler</span></code></a> before calling <code class="docutils literal notranslate"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal notranslate"><span class="pre">normalize=False</span></code>.</p>
</dd>
<dt><strong>max_iter</strong><span class="classifier">int, default=1000</span></dt><dd><p>The maximum number of iterations</p>
</dd>
<dt><strong>tol</strong><span class="classifier">float, default=1e-7</span></dt><dd><p>Stopping criterion. Convergence tolerance for the <code class="docutils literal notranslate"><span class="pre">copt</span></code> proximal gradient solver</p>
</dd>
<dt><strong>scoring</strong><span class="classifier">callable, default=None</span></dt><dd><p>A string (see sklearn model evaluation documentation) or a scorer
callable object / function with signature <code class="docutils literal notranslate"><span class="pre">scorer(estimator,</span> <span class="pre">X,</span> <span class="pre">y)</span></code>.
For a list of scoring functions that can be used, look at
<cite>sklearn.metrics</cite>. The default scoring option is “r2”.</p>
</dd>
<dt><strong>cv</strong><span class="classifier">int, cross-validation generator or iterable, default=None</span></dt><dd><p>Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul class="simple">
<li><p>None, to use the default 5-fold cross-validation,</p></li>
<li><p>int, to specify the number of folds.</p></li>
<li><p>an sklearn <a class="reference external" href="https://scikit-learn.org/stable/glossary.html#term-cv-splitter">CV splitter</a>,</p></li>
<li><p>An iterable yielding (train, test) splits as arrays of indices.</p></li>
</ul>
<p>For int/None inputs, <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold" title="(in scikit-learn v1.1)"><code class="docutils literal notranslate"><span class="pre">sklearn.model_selection.KFold</span></code></a> is used.</p>
<p>Refer to the <a class="reference external" href="https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation" title="(in scikit-learn v1.1)"><span class="xref std std-ref">scikit-learn User Guide</span></a> for the various cross-validation
strategies that can be used here.</p>
</dd>
<dt><strong>copy_X</strong><span class="classifier">bool, default=True</span></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">bool or int, default=0</span></dt><dd><p>Amount of verbosity.</p>
</dd>
<dt><strong>n_jobs</strong><span class="classifier">int, default=None</span></dt><dd><p>Number of CPUs to use during the cross validation.
<code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <a class="reference external" href="https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend" title="(in joblib v1.2.0.dev0)"><code class="docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code></a> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors.</p>
</dd>
<dt><strong>tuning_strategy</strong><span class="classifier">[“grid”, “bayes”], default=”grid”</span></dt><dd><p>Hyperparameter tuning strategy to use. If <code class="docutils literal notranslate"><span class="pre">tuning_strategy</span> <span class="pre">==</span> <span class="pre">&quot;grid&quot;</span></code>,
then evaluate all parameter points on the <code class="docutils literal notranslate"><span class="pre">l1_ratio</span></code> and <code class="docutils literal notranslate"><span class="pre">alphas</span></code> grid,
using warm start to evaluate different <code class="docutils literal notranslate"><span class="pre">alpha</span></code> values along the
regularization path. If <code class="docutils literal notranslate"><span class="pre">tuning_strategy</span> <span class="pre">==</span> <span class="pre">&quot;bayes&quot;</span></code>, then a fixed
number of parameter settings is sampled using <code class="docutils literal notranslate"><span class="pre">skopt.BayesSearchCV</span></code>.
The fixed number of settings is set by <code class="docutils literal notranslate"><span class="pre">n_bayes_iter</span></code>. The
<code class="docutils literal notranslate"><span class="pre">l1_ratio</span></code> setting is sampled uniformly from the minimum and maximum
of the input <code class="docutils literal notranslate"><span class="pre">l1_ratio</span></code> parameter. The <code class="docutils literal notranslate"><span class="pre">alpha</span></code> setting is sampled
log-uniformly either from the maximum and minumum of the input <code class="docutils literal notranslate"><span class="pre">alphas</span></code>
parameter, if provided or from <code class="docutils literal notranslate"><span class="pre">eps</span></code> * max_alpha to max_alpha where
max_alpha is a conservative estimate of the maximum alpha for which the
solution coefficients are non-trivial.</p>
</dd>
<dt><strong>n_bayes_iter</strong><span class="classifier">int, default=50</span></dt><dd><p>Number of parameter settings that are sampled if using Bayes search
for hyperparameter optimization. <code class="docutils literal notranslate"><span class="pre">n_bayes_iter</span></code> trades off runtime
vs quality of the solution. Consider increasing <code class="docutils literal notranslate"><span class="pre">n_bayes_points</span></code> if
you want to try more parameter settings in parallel.</p>
</dd>
<dt><strong>n_bayes_points</strong><span class="classifier">int, default=1</span></dt><dd><p>Number of parameter settings to sample in parallel if using Bayes
search for hyperparameter optimization. If this does not align with
<code class="docutils literal notranslate"><span class="pre">n_bayes_iter</span></code>, the last iteration will sample fewer points.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt><dd><p>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</p>
</dd>
<dt><strong>suppress_solver_warnings</strong><span class="classifier">bool, default=True</span></dt><dd><p>If True, suppress warnings from BayesSearchCV when the objective is
evaluated at the same point multiple times. Setting this to False,
may be useful for debugging.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#groupyr.sgl_path" title="groupyr.sgl_path"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sgl_path</span></code></a></dt><dd></dd>
<dt><a class="reference internal" href="#groupyr.SGL" title="groupyr.SGL"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SGL</span></code></a></dt><dd></dd>
</dl>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>alpha_</strong><span class="classifier">float</span></dt><dd><p>The amount of penalization chosen by cross validation</p>
</dd>
<dt><strong>l1_ratio_</strong><span class="classifier">float</span></dt><dd><p>The compromise between l1 and l2 penalization chosen by
cross validation</p>
</dd>
<dt><strong>coef_</strong><span class="classifier">ndarray of shape (n_features,) or (n_targets, n_features)</span></dt><dd><p>Parameter vector (w in the cost function formula),</p>
</dd>
<dt><strong>intercept_</strong><span class="classifier">float or ndarray of shape (n_targets, n_features)</span></dt><dd><p>Independent term in the decision function.</p>
</dd>
<dt><strong>scoring_path_</strong><span class="classifier">ndarray of shape (n_l1_ratio, n_alpha, n_folds)</span></dt><dd><p>Mean square error for the test set on each fold, varying l1_ratio and
alpha.</p>
</dd>
<dt><strong>alphas_</strong><span class="classifier">ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)</span></dt><dd><p>The grid of alphas used for fitting, for each l1_ratio.</p>
</dd>
<dt><strong>n_iter_</strong><span class="classifier">int</span></dt><dd><p>number of iterations run by the proximal gradient descent solver to
reach the specified tolerance for the optimal alpha.</p>
</dd>
<dt><strong>bayes_optimizer_</strong><span class="classifier">skopt.BayesSearchCV instance or None</span></dt><dd><p>The BayesSearchCV instance used for hyperparameter optimization if
<code class="docutils literal notranslate"><span class="pre">tuning_strategy</span> <span class="pre">==</span> <span class="pre">&quot;bayes&quot;</span></code>. If <code class="docutils literal notranslate"><span class="pre">tuning_strategy</span> <span class="pre">==</span> <span class="pre">&quot;grid&quot;</span></code>,
then this attribute is None.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="groupyr.LogisticSGLCV">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">groupyr.</span></span><span class="sig-name descname"><span class="pre">LogisticSGLCV</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">l1_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_l2_by</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'group_length'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-07</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tuning_strategy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'grid'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_bayes_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_bayes_points</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">suppress_solver_warnings</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/groupyr/logistic.html#LogisticSGLCV"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#groupyr.LogisticSGLCV" title="Permalink to this definition"></a></dt>
<dd><p>Iterative Logistic SGL model fitting along a regularization path.</p>
<p>See the scikit-learn glossary entry for <a class="reference external" href="https://scikit-learn.org/stable/glossary.html#term-cross-validation-estimator">cross-validation estimator</a></p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>l1_ratio</strong><span class="classifier">float or list of float, default=1.0</span></dt><dd><p>float between 0 and 1 passed to SGL (scaling between group lasso and
lasso penalties). For <code class="docutils literal notranslate"><span class="pre">l1_ratio</span> <span class="pre">=</span> <span class="pre">0</span></code> the penalty is the group lasso
penalty. For <code class="docutils literal notranslate"><span class="pre">l1_ratio</span> <span class="pre">=</span> <span class="pre">1</span></code> it is the lasso penalty. For <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;</span>
<span class="pre">l1_ratio</span> <span class="pre">&lt;</span> <span class="pre">1</span></code>, the penalty is a combination of group lasso and
lasso. This parameter can be a list, in which case the different
values are tested by cross-validation and the one giving the best
prediction score is used. Note that a good choice of list of values
will depend on the problem. For problems where we expect strong
overall sparsity and would like to encourage grouping, put more
values close to 1 (i.e. Lasso). In contrast, if we expect strong
group-wise sparsity, but only mild sparsity within groups, put more
values close to 0 (i.e. group lasso).</p>
</dd>
<dt><strong>groups</strong><span class="classifier">list of numpy.ndarray</span></dt><dd><p>list of arrays of non-overlapping indices for each group. For
example, if nine features are grouped into equal contiguous groups of
three, then groups would be <code class="docutils literal notranslate"><span class="pre">[array([0,</span> <span class="pre">1,</span> <span class="pre">2]),</span> <span class="pre">array([3,</span> <span class="pre">4,</span> <span class="pre">5]),</span>
<span class="pre">array([6,</span> <span class="pre">7,</span> <span class="pre">8])]</span></code>. If the feature matrix contains a bias or
intercept feature, do not include it as a group. If None, all
features will belong to one group. We set groups in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> so
that it can be reused in model selection and CV routines.</p>
</dd>
<dt><strong>scale_l2_by</strong><span class="classifier">[“group_length”, None], default=”group_length”</span></dt><dd><p>Scaling technique for the group-wise L2 penalty.
By default, <code class="docutils literal notranslate"><span class="pre">scale_l2_by=&quot;group_length</span></code> and the L2 penalty is
scaled by the square root of the group length so that each variable
has the same effect on the penalty. This may not be appropriate for
one-hot encoded features and <code class="docutils literal notranslate"><span class="pre">scale_l2_by=None</span></code> would be more
appropriate for that case. <code class="docutils literal notranslate"><span class="pre">scale_l2_by=None</span></code> will also reproduce
ElasticNet results when all features belong to one group.</p>
</dd>
<dt><strong>eps</strong><span class="classifier">float, default=1e-3</span></dt><dd><p>Length of the path. <code class="docutils literal notranslate"><span class="pre">eps=1e-3</span></code> means that
<code class="docutils literal notranslate"><span class="pre">alpha_min</span> <span class="pre">/</span> <span class="pre">alpha_max</span> <span class="pre">=</span> <span class="pre">1e-3</span></code>.</p>
</dd>
<dt><strong>n_alphas</strong><span class="classifier">int, default=100</span></dt><dd><p>Number of alphas along the regularization path, used for each l1_ratio.</p>
</dd>
<dt><strong>alphas</strong><span class="classifier">ndarray, default=None</span></dt><dd><p>List of alphas where to compute the models.
If None alphas are set automatically</p>
</dd>
<dt><strong>fit_intercept</strong><span class="classifier">bool, default=True</span></dt><dd><p>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(i.e. data is expected to be centered).</p>
</dd>
<dt><strong>normalize</strong><span class="classifier">bool, default=False</span></dt><dd><p>This parameter is ignored when <code class="docutils literal notranslate"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler" title="(in scikit-learn v1.1)"><code class="docutils literal notranslate"><span class="pre">sklearn.preprocessing.StandardScaler</span></code></a> before calling <code class="docutils literal notranslate"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal notranslate"><span class="pre">normalize=False</span></code>.</p>
</dd>
<dt><strong>max_iter</strong><span class="classifier">int, default=1000</span></dt><dd><p>The maximum number of iterations</p>
</dd>
<dt><strong>tol</strong><span class="classifier">float, default=1e-7</span></dt><dd><p>Stopping criterion. Convergence tolerance for the <code class="docutils literal notranslate"><span class="pre">copt</span></code> proximal gradient solver</p>
</dd>
<dt><strong>scoring</strong><span class="classifier">callable, default=None</span></dt><dd><p>A string (see sklearn model evaluation documentation) or a scorer
callable object / function with signature <code class="docutils literal notranslate"><span class="pre">scorer(estimator,</span> <span class="pre">X,</span> <span class="pre">y)</span></code>.
For a list of scoring functions that can be used, look at
<cite>sklearn.metrics</cite>. The default scoring option used is accuracy_score.</p>
</dd>
<dt><strong>cv</strong><span class="classifier">int, cross-validation generator or iterable, default=None</span></dt><dd><p>Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul class="simple">
<li><p>None, to use the default 5-fold cross-validation,</p></li>
<li><p>int, to specify the number of folds.</p></li>
<li><p>an sklearn <a class="reference external" href="https://scikit-learn.org/stable/glossary.html#term-cv-splitter">CV splitter</a>,</p></li>
<li><p>An iterable yielding (train, test) splits as arrays of indices.</p></li>
</ul>
<p>For int/None inputs, <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold" title="(in scikit-learn v1.1)"><code class="docutils literal notranslate"><span class="pre">sklearn.model_selection.StratifiedKFold</span></code></a> is used.</p>
<p>Refer to the <a class="reference external" href="https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation" title="(in scikit-learn v1.1)"><span class="xref std std-ref">scikit-learn User Guide</span></a> for the various cross-validation
strategies that can be used here.</p>
</dd>
<dt><strong>copy_X</strong><span class="classifier">bool, default=True</span></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">bool or int, default=False</span></dt><dd><p>Amount of verbosity.</p>
</dd>
<dt><strong>n_jobs</strong><span class="classifier">int, default=None</span></dt><dd><p>Number of CPUs to use during the cross validation.
<code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <a class="reference external" href="https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend" title="(in joblib v1.2.0.dev0)"><code class="docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code></a> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors.</p>
</dd>
<dt><strong>tuning_strategy</strong><span class="classifier">[“grid”, “bayes”], default=”grid”</span></dt><dd><p>Hyperparameter tuning strategy to use. If <code class="docutils literal notranslate"><span class="pre">tuning_strategy</span> <span class="pre">==</span> <span class="pre">&quot;grid&quot;</span></code>,
then evaluate all parameter points on the <code class="docutils literal notranslate"><span class="pre">l1_ratio</span></code> and <code class="docutils literal notranslate"><span class="pre">alphas</span></code> grid,
using warm start to evaluate different <code class="docutils literal notranslate"><span class="pre">alpha</span></code> values along the
regularization path. If <code class="docutils literal notranslate"><span class="pre">tuning_strategy</span> <span class="pre">==</span> <span class="pre">&quot;bayes&quot;</span></code>, then a fixed
number of parameter settings is sampled using <code class="docutils literal notranslate"><span class="pre">skopt.BayesSearchCV</span></code>.
The fixed number of settings is set by <code class="docutils literal notranslate"><span class="pre">n_bayes_iter</span></code>. The
<code class="docutils literal notranslate"><span class="pre">l1_ratio</span></code> setting is sampled uniformly from the minimum and maximum
of the input <code class="docutils literal notranslate"><span class="pre">l1_ratio</span></code> parameter. The <code class="docutils literal notranslate"><span class="pre">alpha</span></code> setting is sampled
log-uniformly either from the maximum and minumum of the input <code class="docutils literal notranslate"><span class="pre">alphas</span></code>
parameter, if provided or from <code class="docutils literal notranslate"><span class="pre">eps</span></code> * max_alpha to max_alpha where
max_alpha is a conservative estimate of the maximum alpha for which the
solution coefficients are non-trivial.</p>
</dd>
<dt><strong>n_bayes_iter</strong><span class="classifier">int, default=50</span></dt><dd><p>Number of parameter settings that are sampled if using Bayes search
for hyperparameter optimization. <code class="docutils literal notranslate"><span class="pre">n_bayes_iter</span></code> trades off runtime
vs quality of the solution. Consider increasing <code class="docutils literal notranslate"><span class="pre">n_bayes_points</span></code> if
you want to try more parameter settings in parallel.</p>
</dd>
<dt><strong>n_bayes_points</strong><span class="classifier">int, default=1</span></dt><dd><p>Number of parameter settings to sample in parallel if using Bayes
search for hyperparameter optimization. If this does not align with
<code class="docutils literal notranslate"><span class="pre">n_bayes_iter</span></code>, the last iteration will sample fewer points.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt><dd><p>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</p>
</dd>
<dt><strong>suppress_solver_warnings</strong><span class="classifier">bool, default=True</span></dt><dd><p>If True, suppress warnings from BayesSearchCV when the objective is
evaluated at the same point multiple times. Setting this to False,
may be useful for debugging.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">logistic_sgl_path</span></code></dt><dd></dd>
<dt><a class="reference internal" href="#groupyr.LogisticSGL" title="groupyr.LogisticSGL"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LogisticSGL</span></code></a></dt><dd></dd>
</dl>
</div>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>alpha_</strong><span class="classifier">float</span></dt><dd><p>The amount of penalization chosen by cross validation</p>
</dd>
<dt><strong>l1_ratio_</strong><span class="classifier">float</span></dt><dd><p>The compromise between l1 and l2 penalization chosen by
cross validation</p>
</dd>
<dt><strong>classes_</strong><span class="classifier">ndarray of shape (n_classes, )</span></dt><dd><p>A list of class labels known to the classifier.</p>
</dd>
<dt><strong>coef_</strong><span class="classifier">array of shape (n_features,)</span></dt><dd><p>Estimated coefficients for the linear predictor (<cite>X &#64; coef_ +
intercept_</cite>).</p>
</dd>
<dt><strong>intercept_</strong><span class="classifier">float</span></dt><dd><p>Intercept (a.k.a. bias) added to linear predictor.</p>
</dd>
<dt><strong>scoring_path_</strong><span class="classifier">ndarray of shape (n_l1_ratio, n_alpha, n_folds)</span></dt><dd><p>Classification score for the test set on each fold, varying l1_ratio and
alpha.</p>
</dd>
<dt><strong>alphas_</strong><span class="classifier">ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)</span></dt><dd><p>The grid of alphas used for fitting, for each l1_ratio.</p>
</dd>
<dt><strong>n_iter_</strong><span class="classifier">int</span></dt><dd><p>number of iterations run by the proximal gradient descent solver to
reach the specified tolerance for the optimal alpha.</p>
</dd>
<dt><strong>bayes_optimizer_</strong><span class="classifier">skopt.BayesSearchCV instance or None</span></dt><dd><p>The BayesSearchCV instance used for hyperparameter optimization if
<code class="docutils literal notranslate"><span class="pre">tuning_strategy</span> <span class="pre">==</span> <span class="pre">&quot;bayes&quot;</span></code>. If <code class="docutils literal notranslate"><span class="pre">tuning_strategy</span> <span class="pre">==</span> <span class="pre">&quot;grid&quot;</span></code>,
then this attribute is None.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</section>
<section id="dataset-generation">
<h2>Dataset Generation<a class="headerlink" href="#dataset-generation" title="Permalink to this headline"></a></h2>
<p>Use these functions to generate synthetic sparse grouped data.</p>
<dl class="py function">
<dt class="sig sig-object py" id="groupyr.datasets.make_group_classification">
<span class="sig-prename descclassname"><span class="pre">groupyr.datasets.</span></span><span class="sig-name descname"><span class="pre">make_group_classification</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_informative_groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_features_per_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_informative_per_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_redundant_per_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_repeated_per_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_clusters_per_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">flip_y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_sep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hypercube</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shift</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">useful_indices</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/groupyr/datasets.html#make_group_classification"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#groupyr.datasets.make_group_classification" title="Permalink to this definition"></a></dt>
<dd><p>Generate a random n-class sparse group classification problem.</p>
<p>This function is a generalization of sklearn.datasets.make_classification
to feature matrices with grouped covariates. Prior to shuffling, <code class="docutils literal notranslate"><span class="pre">X</span></code>
stacks a number of these primary “informative”
features, “redundant” linear combinations of these, “repeated” duplicates
of sampled features, and arbitrary noise for and remaining features.
This method uses sklearn.datasets.make_classification to construct a
giant unshuffled classification problem of size
<code class="docutils literal notranslate"><span class="pre">n_groups</span> <span class="pre">*</span> <span class="pre">n_features_per_group</span></code> and then distributes the returned
features to each group. It then optionally shuffles each group.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_samples</strong><span class="classifier">int, optional (default=100)</span></dt><dd><p>The number of samples.</p>
</dd>
<dt><strong>n_groups</strong><span class="classifier">int, optional (default=10)</span></dt><dd><p>The number of feature groups.</p>
</dd>
<dt><strong>n_informative_groups</strong><span class="classifier">int, optional (default=2)</span></dt><dd><p>The total number of informative groups. All other groups will be
just noise.</p>
</dd>
<dt><strong>n_features_per_group</strong><span class="classifier">int, optional (default=20)</span></dt><dd><p>The total number of features_per_group. These comprise <cite>n_informative</cite>
informative features, <cite>n_redundant</cite> redundant features, <cite>n_repeated</cite>
duplicated features and <cite>n_features-n_informative-n_redundant-
n_repeated</cite> useless features drawn at random.</p>
</dd>
<dt><strong>n_informative_per_group</strong><span class="classifier">int, optional (default=2)</span></dt><dd><p>The number of informative features_per_group. Each class is composed
of a number of gaussian clusters each located around the vertices of a
hypercube in a subspace of dimension <cite>n_informative_per_group</cite>. For
each cluster, informative features are drawn independently from
N(0, 1) and then randomly linearly combined within each cluster in
order to add covariance. The clusters are then placed on the vertices
of the hypercube.</p>
</dd>
<dt><strong>n_redundant_per_group</strong><span class="classifier">int, optional (default=2)</span></dt><dd><p>The number of redundant features per group. These features are
generated as random linear combinations of the informative features.</p>
</dd>
<dt><strong>n_repeated_per_group</strong><span class="classifier">int, optional (default=0)</span></dt><dd><p>The number of duplicated features per group, drawn randomly from the
informative and the redundant features.</p>
</dd>
<dt><strong>n_classes</strong><span class="classifier">int, optional (default=2)</span></dt><dd><p>The number of classes (or labels) of the classification problem.</p>
</dd>
<dt><strong>n_clusters_per_class</strong><span class="classifier">int, optional (default=2)</span></dt><dd><p>The number of clusters per class.</p>
</dd>
<dt><strong>weights</strong><span class="classifier">list of floats or None (default=None)</span></dt><dd><p>The proportions of samples assigned to each class. If None, then
classes are balanced. Note that if <cite>len(weights) == n_classes - 1</cite>,
then the last class weight is automatically inferred.
More than <cite>n_samples</cite> samples may be returned if the sum of <cite>weights</cite>
exceeds 1.</p>
</dd>
<dt><strong>flip_y</strong><span class="classifier">float, optional (default=0.01)</span></dt><dd><p>The fraction of samples whose class are randomly exchanged. Larger
values introduce noise in the labels and make the classification
task harder.</p>
</dd>
<dt><strong>class_sep</strong><span class="classifier">float, optional (default=1.0)</span></dt><dd><p>The factor multiplying the hypercube size.  Larger values spread
out the clusters/classes and make the classification task easier.</p>
</dd>
<dt><strong>hypercube</strong><span class="classifier">boolean, optional (default=True)</span></dt><dd><p>If True, the clusters are put on the vertices of a hypercube. If
False, the clusters are put on the vertices of a random polytope.</p>
</dd>
<dt><strong>shift</strong><span class="classifier">float, array of shape [n_features] or None, optional (default=0.0)</span></dt><dd><p>Shift features by the specified value. If None, then features
are shifted by a random value drawn in [-class_sep, class_sep].</p>
</dd>
<dt><strong>scale</strong><span class="classifier">float, array of shape [n_features] or None, optional (default=1.0)</span></dt><dd><p>Multiply features by the specified value. If None, then features
are scaled by a random value drawn in [1, 100]. Note that scaling
happens after shifting.</p>
</dd>
<dt><strong>shuffle</strong><span class="classifier">boolean, optional (default=True)</span></dt><dd><p>Shuffle the samples and the features.</p>
</dd>
<dt><strong>useful_indices</strong><span class="classifier">boolean, optional (default=False)</span></dt><dd><p>If True, a boolean array indicating useful features is returned</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt><dd><p>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array of shape [n_samples, n_features]</span></dt><dd><p>The generated samples.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array of shape [n_samples]</span></dt><dd><p>The integer labels for class membership of each sample.</p>
</dd>
<dt><strong>groups</strong><span class="classifier">list of arrays</span></dt><dd><p>Each element is an array of feature indices that belong to that group</p>
</dd>
<dt><strong>indices</strong><span class="classifier">array of shape [n_features]</span></dt><dd><p>A boolean array indicating which features are useful. Returned only
if <cite>useful_indices</cite> is True.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn.datasets.make_classification" title="(in scikit-learn v1.1)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.datasets.make_classification</span></code></a></dt><dd><p>non-group-sparse version</p>
</dd>
<dt><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs" title="(in scikit-learn v1.1)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.datasets.make_blobs</span></code></a></dt><dd><p>simplified variant</p>
</dd>
<dt><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_multilabel_classification.html#sklearn.datasets.make_multilabel_classification" title="(in scikit-learn v1.1)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.datasets.make_multilabel_classification</span></code></a></dt><dd><p>unrelated generator for multilabel tasks</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>The algorithm is adapted from Guyon [1] and was designed to generate
the “Madelon” dataset.</p>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="r89cbf7c972e0-1"><span class="brackets">1</span></dt>
<dd><p>I. Guyon, “Design of experiments for the NIPS 2003 variable
selection benchmark”, 2003.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="groupyr.datasets.make_group_regression">
<span class="sig-prename descclassname"><span class="pre">groupyr.datasets.</span></span><span class="sig-name descname"><span class="pre">make_group_regression</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_informative_groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_features_per_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_informative_per_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">effective_rank</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">noise</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shift</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">coef</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/groupyr/datasets.html#make_group_regression"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#groupyr.datasets.make_group_regression" title="Permalink to this definition"></a></dt>
<dd><p>Generate a sparse group regression problem.</p>
<p>This function is a generalization of sklearn.datasets.make_regression
to feature matrices with grouped covariates. Prior to shuffling, <code class="docutils literal notranslate"><span class="pre">X</span></code>
stacks a number of these primary “informative”
features, and arbitrary noise for and remaining features.
This method uses sklearn.datasets.make_regression to construct a
giant unshuffled regression problem of size
<code class="docutils literal notranslate"><span class="pre">n_groups</span> <span class="pre">*</span> <span class="pre">n_features_per_group</span></code> and then distributes the returned
features to each group. It then optionally shuffles each group.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_samples</strong><span class="classifier">int, optional (default=100)</span></dt><dd><p>The number of samples.</p>
</dd>
<dt><strong>n_groups</strong><span class="classifier">int, optional (default=10)</span></dt><dd><p>The number of feature groups.</p>
</dd>
<dt><strong>n_informative_groups</strong><span class="classifier">int, optional (default=2)</span></dt><dd><p>The total number of informative groups. All other groups will be
just noise.</p>
</dd>
<dt><strong>n_features_per_group</strong><span class="classifier">int, optional (default=20)</span></dt><dd><p>The total number of features_per_group. These comprise <cite>n_informative</cite>
informative features, and <cite>n_features-n_informative</cite> useless
features drawn at random.</p>
</dd>
<dt><strong>n_informative_per_group</strong><span class="classifier">int, optional (default=2)</span></dt><dd><p>The number of informative features_per_group that have a
non-zero regression coefficient.</p>
</dd>
<dt><strong>effective_rank</strong><span class="classifier">int or None, optional (default=None)</span></dt><dd><p>If not None, provides the number of singular vectors to explain the
input data.</p>
</dd>
<dt><strong>noise</strong><span class="classifier">float, optional (default=0.0)</span></dt><dd><p>The standard deviation of the gaussian noise applied to the output.</p>
</dd>
<dt><strong>shuffle</strong><span class="classifier">boolean, optional (default=False)</span></dt><dd><p>Shuffle the samples and the features.</p>
</dd>
<dt><strong>coef</strong><span class="classifier">boolean, optional (default=False)</span></dt><dd><p>If True, returns coefficient values used to generate samples via
sklearn.datasets.make_regression.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt><dd><p>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array of shape [n_samples, n_features]</span></dt><dd><p>The generated samples.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array of shape [n_samples]</span></dt><dd><p>The integer labels for class membership of each sample.</p>
</dd>
<dt><strong>groups</strong><span class="classifier">list of arrays</span></dt><dd><p>Each element is an array of feature indices that belong to that group</p>
</dd>
<dt><strong>coef</strong><span class="classifier">array of shape [n_features]</span></dt><dd><p>A numpy array containing true regression coefficient values. Returned only if <cite>coef</cite> is True.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html#sklearn.datasets.make_regression" title="(in scikit-learn v1.1)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.datasets.make_regression</span></code></a></dt><dd><p>non-group-sparse version</p>
</dd>
</dl>
</div>
</dd></dl>

</section>
<section id="regularization-paths">
<h2>Regularization Paths<a class="headerlink" href="#regularization-paths" title="Permalink to this headline"></a></h2>
<p>Use these functions to compute regression coefficients along a regularization path.</p>
<dl class="py function">
<dt class="sig sig-object py" id="groupyr.sgl_path">
<span class="sig-prename descclassname"><span class="pre">groupyr.</span></span><span class="sig-name descname"><span class="pre">sgl_path</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">l1_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_l2_by</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'group_length'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Xy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_n_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/groupyr/sgl.html#sgl_path"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#groupyr.sgl_path" title="Permalink to this definition"></a></dt>
<dd><p>Compute sparse group lasso path.</p>
<p>We use the previous solution as the initial guess for subsequent alpha values</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, sparse matrix} of shape (n_samples, n_features)</span></dt><dd><p>Training data. Pass directly as Fortran-contiguous data to avoid
unnecessary memory duplication.</p>
</dd>
<dt><strong>y</strong><span class="classifier">{array-like, sparse matrix} of shape (n_samples,)</span></dt><dd><p>Target values.</p>
</dd>
<dt><strong>l1_ratio</strong><span class="classifier">float, default=0.5</span></dt><dd><p>Number between 0 and 1 passed to SGL estimator (scaling between the
group lasso and lasso penalties). <code class="docutils literal notranslate"><span class="pre">l1_ratio=1</span></code> corresponds to the
Lasso.</p>
</dd>
<dt><strong>groups</strong><span class="classifier">list of numpy.ndarray</span></dt><dd><p>list of arrays of non-overlapping indices for each group. For
example, if nine features are grouped into equal contiguous groups of
three, then groups would be <code class="docutils literal notranslate"><span class="pre">[array([0,</span> <span class="pre">1,</span> <span class="pre">2]),</span> <span class="pre">array([3,</span> <span class="pre">4,</span> <span class="pre">5]),</span>
<span class="pre">array([6,</span> <span class="pre">7,</span> <span class="pre">8])]</span></code>. If the feature matrix contains a bias or
intercept feature, do not include it as a group. If None, all
features will belong to one group.</p>
</dd>
<dt><strong>scale_l2_by</strong><span class="classifier">[“group_length”, None], default=”group_length”</span></dt><dd><p>Scaling technique for the group-wise L2 penalty.
By default, <code class="docutils literal notranslate"><span class="pre">scale_l2_by=&quot;group_length</span></code> and the L2 penalty is
scaled by the square root of the group length so that each variable
has the same effect on the penalty. This may not be appropriate for
one-hot encoded features and <code class="docutils literal notranslate"><span class="pre">scale_l2_by=None</span></code> would be more
appropriate for that case. <code class="docutils literal notranslate"><span class="pre">scale_l2_by=None</span></code> will also reproduce
ElasticNet results when all features belong to one group.</p>
</dd>
<dt><strong>eps</strong><span class="classifier">float, default=1e-3</span></dt><dd><p>Length of the path. <code class="docutils literal notranslate"><span class="pre">eps=1e-3</span></code> means that
<code class="docutils literal notranslate"><span class="pre">alpha_min</span> <span class="pre">/</span> <span class="pre">alpha_max</span> <span class="pre">=</span> <span class="pre">1e-3</span></code>.</p>
</dd>
<dt><strong>n_alphas</strong><span class="classifier">int, default=100</span></dt><dd><p>Number of alphas along the regularization path.</p>
</dd>
<dt><strong>alphas</strong><span class="classifier">ndarray, default=None</span></dt><dd><p>List of alphas where to compute the models.
If None alphas are set automatically.</p>
</dd>
<dt><strong>Xy</strong><span class="classifier">array-like of shape (n_features,), default=None</span></dt><dd><p>Xy = np.dot(X.T, y) that can be precomputed. If supplying <code class="docutils literal notranslate"><span class="pre">Xy</span></code>,
prevent train/test leakage by ensuring the <code class="docutils literal notranslate"><span class="pre">Xy</span></code> is precomputed
using only training data.</p>
</dd>
<dt><strong>normalize</strong><span class="classifier">bool, default=False</span></dt><dd><p>This parameter is ignored when <code class="docutils literal notranslate"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.preprocessing.StandardScaler</span></code></a> before calling <code class="docutils literal notranslate"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal notranslate"><span class="pre">normalize=False</span></code>.</p>
</dd>
<dt><strong>copy_X</strong><span class="classifier">bool, default=True</span></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">bool or int, default=False</span></dt><dd><p>Amount of verbosity.</p>
</dd>
<dt><strong>check_input</strong><span class="classifier">bool, default=True</span></dt><dd><p>Skip input validation checks, assuming there are handled by the
caller when check_input=False.</p>
</dd>
<dt><strong>**params</strong><span class="classifier">kwargs</span></dt><dd><p>Keyword arguments passed to the SGL estimator</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>coefs</strong><span class="classifier">ndarray of shape (n_features, n_alphas) or (n_features + 1, n_alphas)</span></dt><dd><p>List of coefficients for the Logistic Regression model. If
fit_intercept is set to True then the first dimension will be
n_features + 1, where the last item represents the intercept.</p>
</dd>
<dt><strong>alphas</strong><span class="classifier">ndarray of shape (n_alphas,)</span></dt><dd><p>The alphas along the path where models are computed.</p>
</dd>
<dt><strong>n_iters</strong><span class="classifier">array of shape (n_alphas,)</span></dt><dd><p>Actual number of iteration for each alpha.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#groupyr.SGL" title="groupyr.SGL"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SGL</span></code></a></dt><dd></dd>
<dt><a class="reference internal" href="#groupyr.SGLCV" title="groupyr.SGLCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SGLCV</span></code></a></dt><dd></dd>
</dl>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="groupyr.logistic.logistic_sgl_path">
<span class="sig-prename descclassname"><span class="pre">groupyr.logistic.</span></span><span class="sig-name descname"><span class="pre">logistic_sgl_path</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">l1_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_l2_by</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'group_length'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Xy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/groupyr/logistic.html#logistic_sgl_path"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#groupyr.logistic.logistic_sgl_path" title="Permalink to this definition"></a></dt>
<dd><p>Compute a Logistic SGL model for a list of regularization parameters.</p>
<p>This is an implementation that uses the result of the previous model
to speed up computations along the regularization path, making it faster
than calling LogisticSGL for the different parameters without warm start.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, sparse matrix} of shape (n_samples, n_features)</span></dt><dd><p>Training data. Pass directly as Fortran-contiguous data to avoid
unnecessary memory duplication.</p>
</dd>
<dt><strong>y</strong><span class="classifier">{array-like, sparse matrix} of shape (n_samples,)</span></dt><dd><p>Target values.</p>
</dd>
<dt><strong>l1_ratio</strong><span class="classifier">float, default=0.5</span></dt><dd><p>Number between 0 and 1 passed to SGL estimator (scaling between the
group lasso and lasso penalties). <code class="docutils literal notranslate"><span class="pre">l1_ratio=1</span></code> corresponds to the
Lasso.</p>
</dd>
<dt><strong>groups</strong><span class="classifier">list of numpy.ndarray</span></dt><dd><p>list of arrays of non-overlapping indices for each group. For
example, if nine features are grouped into equal contiguous groups of
three, then groups would be <code class="docutils literal notranslate"><span class="pre">[array([0,</span> <span class="pre">1,</span> <span class="pre">2]),</span> <span class="pre">array([3,</span> <span class="pre">4,</span> <span class="pre">5]),</span>
<span class="pre">array([6,</span> <span class="pre">7,</span> <span class="pre">8])]</span></code>. If the feature matrix contains a bias or
intercept feature, do not include it as a group. If None, all
features will belong to one group.</p>
</dd>
<dt><strong>scale_l2_by</strong><span class="classifier">[“group_length”, None], default=”group_length”</span></dt><dd><p>Scaling technique for the group-wise L2 penalty.
By default, <code class="docutils literal notranslate"><span class="pre">scale_l2_by=&quot;group_length</span></code> and the L2 penalty is
scaled by the square root of the group length so that each variable
has the same effect on the penalty. This may not be appropriate for
one-hot encoded features and <code class="docutils literal notranslate"><span class="pre">scale_l2_by=None</span></code> would be more
appropriate for that case. <code class="docutils literal notranslate"><span class="pre">scale_l2_by=None</span></code> will also reproduce
ElasticNet results when all features belong to one group.</p>
</dd>
<dt><strong>eps</strong><span class="classifier">float, default=1e-3</span></dt><dd><p>Length of the path. <code class="docutils literal notranslate"><span class="pre">eps=1e-3</span></code> means that
<code class="docutils literal notranslate"><span class="pre">alpha_min</span> <span class="pre">/</span> <span class="pre">alpha_max</span> <span class="pre">=</span> <span class="pre">1e-3</span></code>.</p>
</dd>
<dt><strong>n_alphas</strong><span class="classifier">int, default=100</span></dt><dd><p>Number of alphas along the regularization path.</p>
</dd>
<dt><strong>alphas</strong><span class="classifier">ndarray, default=None</span></dt><dd><p>List of alphas where to compute the models.
If None alphas are set automatically.</p>
</dd>
<dt><strong>Xy</strong><span class="classifier">array-like of shape (n_features,), default=None</span></dt><dd><p>Xy = np.dot(X.T, y) that can be precomputed. If supplying <code class="docutils literal notranslate"><span class="pre">Xy</span></code>,
prevent train/test leakage by ensuring the <code class="docutils literal notranslate"><span class="pre">Xy</span></code> is precomputed
using only training data.</p>
</dd>
<dt><strong>normalize</strong><span class="classifier">bool, default=False</span></dt><dd><p>This parameter is ignored when <code class="docutils literal notranslate"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler" title="(in scikit-learn v1.1)"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.preprocessing.StandardScaler</span></code></a> before calling <code class="docutils literal notranslate"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal notranslate"><span class="pre">normalize=False</span></code>.</p>
</dd>
<dt><strong>copy_X</strong><span class="classifier">bool, default=True</span></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">bool or int, default=False</span></dt><dd><p>Amount of verbosity.</p>
</dd>
<dt><strong>check_input</strong><span class="classifier">bool, default=True</span></dt><dd><p>Skip input validation checks, assuming there are handled by the
caller when check_input=False.</p>
</dd>
<dt><strong>**params</strong><span class="classifier">kwargs</span></dt><dd><p>Keyword arguments passed to the LogisticSGL estimator</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>coefs</strong><span class="classifier">ndarray of shape (n_features, n_alphas) or (n_features + 1, n_alphas)</span></dt><dd><p>List of coefficients for the Logistic Regression model. If
fit_intercept is set to True then the second dimension will be
n_features + 1, where the last item represents the intercept.</p>
</dd>
<dt><strong>alphas</strong><span class="classifier">ndarray</span></dt><dd><p>Grid of alphas used for cross-validation.</p>
</dd>
<dt><strong>n_iters</strong><span class="classifier">array of shape (n_alphas,)</span></dt><dd><p>Actual number of iteration for each alpha.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</section>
<section id="group-transformers">
<h2>Group Transformers<a class="headerlink" href="#group-transformers" title="Permalink to this headline"></a></h2>
<p>These classes perform group-wise transformations on their inputs.</p>
<dl class="py class">
<dt class="sig sig-object py" id="groupyr.transform.GroupExtractor">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">groupyr.transform.</span></span><span class="sig-name descname"><span class="pre">GroupExtractor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">select</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">select_intersection</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/groupyr/transform.html#GroupExtractor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#groupyr.transform.GroupExtractor" title="Permalink to this definition"></a></dt>
<dd><p>An sklearn-compatible group extractor.</p>
<p>Given a sequence of all group indices and a subsequence of desired
group indices, this transformer returns the columns of the feature
matrix, <cite>X</cite>, that are in the desired subgroups.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>select</strong><span class="classifier">numpy.ndarray, int, or str, optional</span></dt><dd><p>subsequence of desired groups to extract from feature matrix
If int or sequence of ints, these will be treated as group indices.
If str or sequence of str, these will be treated as labels for any
level of the (potentially multi-indexed) group names, which must be
specified in <code class="docutils literal notranslate"><span class="pre">group_names</span></code></p>
</dd>
<dt><strong>groups</strong><span class="classifier">list of numpy.ndarray</span></dt><dd><p>list of arrays of non-overlapping indices for each group. For
example, if nine features are grouped into equal contiguous groups of
three, then groups would be <code class="docutils literal notranslate"><span class="pre">[array([0,</span> <span class="pre">1,</span> <span class="pre">2]),</span> <span class="pre">array([3,</span> <span class="pre">4,</span> <span class="pre">5]),</span>
<span class="pre">array([6,</span> <span class="pre">7,</span> <span class="pre">8])]</span></code>. If the feature matrix contains a bias or
intercept feature, do not include it as a group. If None, all
features will belong to one group.</p>
</dd>
<dt><strong>group_names</strong><span class="classifier">sequence of str or sequences, optional</span></dt><dd><p>The names of the groups of X. If this is a sequence of strings, then
this transformer will extract groups whose names match <code class="docutils literal notranslate"><span class="pre">select</span></code>. If
this is a sequence of sequences, then this transformer will extract
groups that have labels that match <code class="docutils literal notranslate"><span class="pre">select</span></code> at any level of their
multi-index.</p>
</dd>
<dt><strong>copy_X</strong><span class="classifier">bool, default=False</span></dt><dd><p>if <code class="docutils literal notranslate"><span class="pre">True</span></code>, X will be copied; else, <code class="docutils literal notranslate"><span class="pre">transform</span></code> may return a view</p>
</dd>
<dt><strong>select_intersection</strong><span class="classifier">bool, default=False</span></dt><dd><p>if <code class="docutils literal notranslate"><span class="pre">True</span></code>, and <code class="docutils literal notranslate"><span class="pre">select</span></code> is a sequence, then <code class="docutils literal notranslate"><span class="pre">transform</span></code> will
return the group intersection of labels in <code class="docutils literal notranslate"><span class="pre">select</span></code>. Otherwise,
<code class="docutils literal notranslate"><span class="pre">transform</span></code> will return the group union.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="groupyr.transform.GroupRemover">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">groupyr.transform.</span></span><span class="sig-name descname"><span class="pre">GroupRemover</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">select</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">select_intersection</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/groupyr/transform.html#GroupRemover"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#groupyr.transform.GroupRemover" title="Permalink to this definition"></a></dt>
<dd><p>An sklearn-compatible group remover.</p>
<p>Given a sequence of all group indices and a subsequence of unwanted
group indices, this transformer returns the columns of the feature
matrix, <cite>X</cite>, that DO NOT include the unwanted subgroups.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>select</strong><span class="classifier">numpy.ndarray, int, or str, optional</span></dt><dd><p>subsequence of desired groups to remove from feature matrix
If int or sequence of ints, these will be treated as group indices.
If str or sequence of str, these will be treated as labels for any
level of the (potentially multi-indexed) group names, which must be
specified in <code class="docutils literal notranslate"><span class="pre">group_names</span></code></p>
</dd>
<dt><strong>groups</strong><span class="classifier">list of numpy.ndarray</span></dt><dd><p>list of arrays of non-overlapping indices for each group. For
example, if nine features are grouped into equal contiguous groups of
three, then groups would be <code class="docutils literal notranslate"><span class="pre">[array([0,</span> <span class="pre">1,</span> <span class="pre">2]),</span> <span class="pre">array([3,</span> <span class="pre">4,</span> <span class="pre">5]),</span>
<span class="pre">array([6,</span> <span class="pre">7,</span> <span class="pre">8])]</span></code>. If the feature matrix contains a bias or
intercept feature, do not include it as a group. If None, all
features will belong to one group.</p>
</dd>
<dt><strong>group_names</strong><span class="classifier">sequence of str or sequences, optional</span></dt><dd><p>The names of the groups of X. If this is a sequence of strings, then
this transformer will remove groups whose names match <code class="docutils literal notranslate"><span class="pre">select</span></code>. If
this is a sequence of sequences, then this transformer will remove
groups that have labels that match <code class="docutils literal notranslate"><span class="pre">select</span></code> at any level of their
multi-index.</p>
</dd>
<dt><strong>copy_X</strong><span class="classifier">bool, default=False</span></dt><dd><p>if <code class="docutils literal notranslate"><span class="pre">True</span></code>, X will be copied; else, <code class="docutils literal notranslate"><span class="pre">transform</span></code> may return a view</p>
</dd>
<dt><strong>select_intersection</strong><span class="classifier">bool, default=False</span></dt><dd><p>if <code class="docutils literal notranslate"><span class="pre">True</span></code>, and <code class="docutils literal notranslate"><span class="pre">select</span></code> is a sequence, then <code class="docutils literal notranslate"><span class="pre">transform</span></code> will
return the group intersection of labels in <code class="docutils literal notranslate"><span class="pre">select</span></code>. Otherwise,
<code class="docutils literal notranslate"><span class="pre">transform</span></code> will return the group union.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="groupyr.transform.GroupShuffler">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">groupyr.transform.</span></span><span class="sig-name descname"><span class="pre">GroupShuffler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">select</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">select_intersection</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/groupyr/transform.html#GroupShuffler"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#groupyr.transform.GroupShuffler" title="Permalink to this definition"></a></dt>
<dd><p>Shuffle some groups of a feature matrix, leaving others as is.</p>
<p>Given a sequence of all group indices and a subsequence of
group indices, this transformer returns the feature
matrix, <cite>X</cite>, with the subset of groups shuffled.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>select</strong><span class="classifier">numpy.ndarray, int, or str, optional</span></dt><dd><p>subsequence of desired groups to shuffle in the feature matrix
If int or sequence of ints, these will be treated as group indices.
If str or sequence of str, these will be treated as labels for any
level of the (potentially multi-indexed) group names, which must be
specified in <code class="docutils literal notranslate"><span class="pre">group_names</span></code></p>
</dd>
<dt><strong>groups</strong><span class="classifier">list of numpy.ndarray</span></dt><dd><p>list of arrays of non-overlapping indices for each group. For
example, if nine features are grouped into equal contiguous groups of
three, then groups would be <code class="docutils literal notranslate"><span class="pre">[array([0,</span> <span class="pre">1,</span> <span class="pre">2]),</span> <span class="pre">array([3,</span> <span class="pre">4,</span> <span class="pre">5]),</span>
<span class="pre">array([6,</span> <span class="pre">7,</span> <span class="pre">8])]</span></code>. If the feature matrix contains a bias or
intercept feature, do not include it as a group. If None, all
features will belong to one group.</p>
</dd>
<dt><strong>group_names</strong><span class="classifier">sequence of str or sequences, optional</span></dt><dd><p>The names of the groups of X. If this is a sequence of strings, then
this transformer will shuffle groups whose names match <code class="docutils literal notranslate"><span class="pre">select</span></code>. If
this is a sequence of sequences, then this transformer will shuffle
groups that have labels that match <code class="docutils literal notranslate"><span class="pre">select</span></code> at any level of their
multi-index.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt><dd><p>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</p>
</dd>
<dt><strong>select_intersection</strong><span class="classifier">bool, default=False</span></dt><dd><p>if <code class="docutils literal notranslate"><span class="pre">True</span></code>, and <code class="docutils literal notranslate"><span class="pre">select</span></code> is a sequence, then <code class="docutils literal notranslate"><span class="pre">transform</span></code> will
return the group intersection of labels in <code class="docutils literal notranslate"><span class="pre">select</span></code>. Otherwise,
<code class="docutils literal notranslate"><span class="pre">transform</span></code> will return the group union.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="groupyr.transform.GroupAggregator">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">groupyr.transform.</span></span><span class="sig-name descname"><span class="pre">GroupAggregator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">func</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kw_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/groupyr/transform.html#GroupAggregator"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#groupyr.transform.GroupAggregator" title="Permalink to this definition"></a></dt>
<dd><p>Aggregate each group of a feature matrix using one or more functions.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>func</strong><span class="classifier">function, str, list or dict</span></dt><dd><p>Function to use for aggregating the data. If a function, it must
accept an <code class="docutils literal notranslate"><span class="pre">axis=1</span></code> parameter. If a string, it must be part of
the numpy namespace. Acceptable input types are</p>
<ul class="simple">
<li><p>function</p></li>
<li><p>string function name</p></li>
<li><p>list of functions and/or function names, e.g. <code class="docutils literal notranslate"><span class="pre">[np.sum,</span> <span class="pre">'mean']</span></code></p></li>
</ul>
<p>If no function is specified, <code class="docutils literal notranslate"><span class="pre">np.mean</span></code> is used.</p>
</dd>
<dt><strong>groups</strong><span class="classifier">list of numpy.ndarray</span></dt><dd><p>list of arrays of non-overlapping indices for each group. For
example, if nine features are grouped into equal contiguous groups of
three, then groups would be <code class="docutils literal notranslate"><span class="pre">[array([0,</span> <span class="pre">1,</span> <span class="pre">2]),</span> <span class="pre">array([3,</span> <span class="pre">4,</span> <span class="pre">5]),</span>
<span class="pre">array([6,</span> <span class="pre">7,</span> <span class="pre">8])]</span></code>. If the feature matrix contains a bias or
intercept feature, do not include it as a group. If None, all
features will belong to one group.</p>
</dd>
<dt><strong>group_names</strong><span class="classifier">sequence of str or sequences, optional</span></dt><dd><p>The names of the groups of X. This parameter has no effect on the output
of the <code class="docutils literal notranslate"><span class="pre">transform()</span></code> method. However, this transformer will keep track
of the transformed feature names using <code class="docutils literal notranslate"><span class="pre">group_names</span></code> if provided.</p>
</dd>
<dt><strong>kw_args</strong></dt><dd><p>Additional keyword arguments to pass to <code class="docutils literal notranslate"><span class="pre">func</span></code>. These will be applied to
all elements of <code class="docutils literal notranslate"><span class="pre">func</span></code> if <code class="docutils literal notranslate"><span class="pre">func</span></code> is a sequence. If “axis” is one of these
keywords, it will be ignored and set to <code class="docutils literal notranslate"><span class="pre">axis=1</span></code>.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Attributes</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>n_features_in_</strong><span class="classifier">int</span></dt><dd><p>The number of features in the feature matrix input to <code class="docutils literal notranslate"><span class="pre">fit()</span></code>.</p>
</dd>
<dt><strong>n_features_out_</strong><span class="classifier">int</span></dt><dd><p>The number of features in the feature matrix output by <code class="docutils literal notranslate"><span class="pre">transform()</span></code>.</p>
</dd>
<dt><strong>groups_</strong><span class="classifier">list of np.ndarray</span></dt><dd><p>The validated group indices used by the transformer</p>
</dd>
<dt><strong>feature_names_out_</strong><span class="classifier">list of str</span></dt><dd><p>A list of the feature names corresponding to columns of the transformed output.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="groupyr.transform.GroupResampler">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">groupyr.transform.</span></span><span class="sig-name descname"><span class="pre">GroupResampler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">resample_to</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kind</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/groupyr/transform.html#GroupResampler"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#groupyr.transform.GroupResampler" title="Permalink to this definition"></a></dt>
<dd><p>Upsample or downsample each group.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>resample_to</strong><span class="classifier">int or float, default=1.0</span></dt><dd><p>If an int, the number of desired resampled features per group.
If a float, the resampling ratio.</p>
</dd>
<dt><strong>groups</strong><span class="classifier">list of numpy.ndarray</span></dt><dd><p>list of arrays of non-overlapping indices for each group. For
example, if nine features are grouped into equal contiguous groups of
three, then groups would be <code class="docutils literal notranslate"><span class="pre">[array([0,</span> <span class="pre">1,</span> <span class="pre">2]),</span> <span class="pre">array([3,</span> <span class="pre">4,</span> <span class="pre">5]),</span>
<span class="pre">array([6,</span> <span class="pre">7,</span> <span class="pre">8])]</span></code>. If the feature matrix contains a bias or
intercept feature, do not include it as a group. If None, all
features will belong to one group.</p>
</dd>
<dt><strong>group_names</strong><span class="classifier">sequence of str or sequences, optional</span></dt><dd><p>The names of the groups of X. This parameter has no effect on the output
of the <code class="docutils literal notranslate"><span class="pre">transform()</span></code> method. However, this transformer will keep track
of the transformed feature names using <code class="docutils literal notranslate"><span class="pre">group_names</span></code> if provided.</p>
</dd>
<dt><strong>kind</strong><span class="classifier">str or int, optional</span></dt><dd><p>Specifies the kind of interpolation as a string or as an integer
specifying the order of the spline interpolator to use.
The string has to be one of ‘linear’, ‘nearest’, ‘nearest-up’, ‘zero’,
‘slinear’, ‘quadratic’, ‘cubic’, ‘previous’, or ‘next’. ‘zero’,
‘slinear’, ‘quadratic’ and ‘cubic’ refer to a spline interpolation of
zeroth, first, second or third order; ‘previous’ and ‘next’ simply
return the previous or next value of the point; ‘nearest-up’ and
‘nearest’ differ when interpolating half-integers (e.g. 0.5, 1.5)
in that ‘nearest-up’ rounds up and ‘nearest’ rounds down. Default
is ‘linear’.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Attributes</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>n_features_in_</strong><span class="classifier">int</span></dt><dd><p>The number of features in the feature matrix input to <code class="docutils literal notranslate"><span class="pre">fit()</span></code>.</p>
</dd>
<dt><strong>n_features_out_</strong><span class="classifier">int</span></dt><dd><p>The number of features in the feature matrix output by <code class="docutils literal notranslate"><span class="pre">transform()</span></code>.</p>
</dd>
<dt><strong>groups_</strong><span class="classifier">list of np.ndarray</span></dt><dd><p>The validated group indices used by the transformer</p>
</dd>
<dt><strong>feature_names_out_</strong><span class="classifier">list of str</span></dt><dd><p>A list of the feature names corresponding to columns of the transformed output.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="getting_help.html" class="btn btn-neutral float-left" title="Getting help using groupyr" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="faq.html" class="btn btn-neutral float-right" title="Frequently Asked Questions" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, Adam Richie-Halford.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>