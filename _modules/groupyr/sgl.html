

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>groupyr.sgl &mdash; groupyr 0.1.2 documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/groupyr.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery-dataframe.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../_static/groupyr-logo.ico"/>
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script src="../../_static/js/copybutton.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home" alt="Documentation Home"> groupyr
          

          
            
            <img src="../../_static/groupyr-logo.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.1.2
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user_guide.html">User guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../auto_examples/index.html">General examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_help.html">Getting help using <em>groupyr</em></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing.html">Contributing to <em>groupyr</em></a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/richford/groupyr">*groupyr* on GitHub</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">groupyr</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>groupyr.sgl</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for groupyr.sgl</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">This module contains regression estimators based on the sparse group lasso</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">delayed</span><span class="p">,</span> <span class="n">effective_n_jobs</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">sparse</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">root_scalar</span>
<span class="kn">from</span> <span class="nn">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">RegressorMixin</span><span class="p">,</span>
    <span class="n">TransformerMixin</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model._base</span> <span class="kn">import</span> <span class="n">LinearModel</span><span class="p">,</span> <span class="n">_preprocess_data</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model._coordinate_descent</span> <span class="kn">import</span> <span class="n">_alpha_grid</span> <span class="k">as</span> <span class="n">_lasso_alpha_grid</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model._coordinate_descent</span> <span class="kn">import</span> <span class="n">_path_residuals</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">check_cv</span>
<span class="kn">from</span> <span class="nn">sklearn.utils.extmath</span> <span class="kn">import</span> <span class="n">safe_sparse_dot</span>
<span class="kn">from</span> <span class="nn">sklearn.utils.fixes</span> <span class="kn">import</span> <span class="n">_joblib_parallel_args</span>
<span class="kn">from</span> <span class="nn">sklearn.utils.validation</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">check_array</span><span class="p">,</span>
    <span class="n">check_is_fitted</span><span class="p">,</span>
    <span class="n">column_or_1d</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">from</span> <span class="nn">._base</span> <span class="kn">import</span> <span class="n">SGLBaseEstimator</span>
<span class="kn">from</span> <span class="nn">._prox</span> <span class="kn">import</span> <span class="n">_soft_threshold</span>
<span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="n">check_groups</span><span class="p">,</span> <span class="n">ProgressParallel</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[]</span>


<span class="k">def</span> <span class="nf">registered</span><span class="p">(</span><span class="n">fn</span><span class="p">):</span>
    <span class="n">__all__</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fn</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fn</span>


<div class="viewcode-block" id="SGL"><a class="viewcode-back" href="../../api.html#groupyr.SGL">[docs]</a><span class="nd">@registered</span>
<span class="k">class</span> <span class="nc">SGL</span><span class="p">(</span><span class="n">SGLBaseEstimator</span><span class="p">,</span> <span class="n">RegressorMixin</span><span class="p">,</span> <span class="n">LinearModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An sklearn compatible sparse group lasso regressor.</span>

<span class="sd">    This solves the sparse group lasso [1]_ problem for a feature matrix</span>
<span class="sd">    partitioned into groups using the proximal gradient descent (PGD)</span>
<span class="sd">    algorithm.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    l1_ratio : float, default=1.0</span>
<span class="sd">        Hyper-parameter : Combination between group lasso and lasso. l1_ratio=0</span>
<span class="sd">        gives the group lasso and l1_ratio=1 gives the lasso.</span>

<span class="sd">    alpha : float, default=1.0</span>
<span class="sd">        Hyper-parameter : overall regularization strength.</span>

<span class="sd">    groups : list of numpy.ndarray</span>
<span class="sd">        list of arrays of non-overlapping indices for each group. For</span>
<span class="sd">        example, if nine features are grouped into equal contiguous groups of</span>
<span class="sd">        three, then groups would be ``[array([0, 1, 2]), array([3, 4, 5]),</span>
<span class="sd">        array([6, 7, 8])]``. If the feature matrix contains a bias or</span>
<span class="sd">        intercept feature, do not include it as a group. If None, all</span>
<span class="sd">        features will belong to one group. We set groups in ``__init__`` so</span>
<span class="sd">        that it can be reused in model selection and CV routines.</span>

<span class="sd">    scale_l2_by : [&quot;group_length&quot;, None], default=&quot;group_length&quot;</span>
<span class="sd">        Scaling technique for the group-wise L2 penalty.</span>
<span class="sd">        By default, ``scale_l2_by=&quot;group_length`` and the L2 penalty is</span>
<span class="sd">        scaled by the square root of the group length so that each variable</span>
<span class="sd">        has the same effect on the penalty. This may not be appropriate for</span>
<span class="sd">        one-hot encoded features and ``scale_l2_by=None`` would be more</span>
<span class="sd">        appropriate for that case. ``scale_l2_by=None`` will also reproduce</span>
<span class="sd">        ElasticNet results when all features belong to one group.</span>

<span class="sd">    fit_intercept : bool, default=True</span>
<span class="sd">        Specifies if a constant (a.k.a. bias or intercept) should be</span>
<span class="sd">        added to the linear predictor (X @ coef + intercept).</span>

<span class="sd">    max_iter : int, default=1000</span>
<span class="sd">        Maximum number of iterations for PGD solver.</span>

<span class="sd">    tol : float, default=1e-7</span>
<span class="sd">        Stopping criterion. Convergence tolerance for PGD algorithm.</span>

<span class="sd">    warm_start : bool, default=False</span>
<span class="sd">        If set to ``True``, reuse the solution of the previous call to ``fit``</span>
<span class="sd">        as initialization for ``coef_`` and ``intercept_``.</span>

<span class="sd">    verbose : int, default=0</span>
<span class="sd">        Verbosity flag for PGD solver. Any positive integer will produce</span>
<span class="sd">        verbose output</span>

<span class="sd">    suppress_solver_warnings : bool, default=True</span>
<span class="sd">        If True, suppress convergence warnings from PGD solver.</span>
<span class="sd">        This is useful for hyperparameter tuning when some combinations</span>
<span class="sd">        of hyperparameters may not converge.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    coef_ : array of shape (n_features,)</span>
<span class="sd">        Estimated coefficients for the linear predictor (`X @ coef_ +</span>
<span class="sd">        intercept_`).</span>

<span class="sd">    intercept_ : float</span>
<span class="sd">        Intercept (a.k.a. bias) added to linear predictor.</span>

<span class="sd">    n_iter_ : int</span>
<span class="sd">        Actual number of iterations used in the solver.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1]  Noah Simon, Jerome Friedman, Trevor Hastie &amp; Robert Tibshirani,</span>
<span class="sd">        &quot;A Sparse-Group Lasso,&quot; Journal of Computational and Graphical</span>
<span class="sd">        Statistics, vol. 22:2, pp. 231-245, 2012</span>
<span class="sd">        DOI: 10.1080/10618600.2012.681250</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;squared_loss&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit a linear model using the sparse group lasso</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>
<span class="sd">            The training input samples.</span>

<span class="sd">        y : array-like, shape (n_samples,)</span>
<span class="sd">            The target values (class labels in classification, real numbers in</span>
<span class="sd">            regression).</span>

<span class="sd">        loss : [&quot;squared_loss&quot;, &quot;huber&quot;]</span>
<span class="sd">            The type of loss function to use in the PGD solver.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Returns self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict targets for test vectors in ``X``.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>
<span class="sd">            The training input samples.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : ndarray, shape (n_samples,)</span>
<span class="sd">            Returns an array of ones.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;is_fitted_&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">safe_sparse_dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dense_output</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span>

    <span class="k">def</span> <span class="nf">_more_tags</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># pylint: disable=no-self-use</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;requires_y&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span></div>


<span class="k">def</span> <span class="nf">_alpha_grid</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span>
    <span class="n">y</span><span class="p">,</span>
    <span class="n">Xy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">groups</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">scale_l2_by</span><span class="o">=</span><span class="s2">&quot;group_length&quot;</span><span class="p">,</span>
    <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
    <span class="n">n_alphas</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">copy_X</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">SGL</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the grid of alpha values for elastic net parameter search</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        Training data. Pass directly as Fortran-contiguous data to avoid</span>
<span class="sd">        unnecessary memory duplication</span>

<span class="sd">    y : ndarray of shape (n_samples,)</span>
<span class="sd">        Target values</span>

<span class="sd">    Xy : array-like of shape (n_features,), default=None</span>
<span class="sd">        Xy = np.dot(X.T, y) that can be precomputed.</span>

<span class="sd">    groups : list of numpy.ndarray</span>
<span class="sd">        list of arrays of non-overlapping indices for each group. For</span>
<span class="sd">        example, if nine features are grouped into equal contiguous groups of</span>
<span class="sd">        three, then groups would be ``[array([0, 1, 2]), array([3, 4, 5]),</span>
<span class="sd">        array([6, 7, 8])]``. If the feature matrix contains a bias or</span>
<span class="sd">        intercept feature, do not include it as a group. If None, all</span>
<span class="sd">        features will belong to one group.</span>

<span class="sd">    scale_l2_by : [&quot;group_length&quot;, None], default=&quot;group_length&quot;</span>
<span class="sd">        Scaling technique for the group-wise L2 penalty.</span>
<span class="sd">        By default, ``scale_l2_by=&quot;group_length`` and the L2 penalty is</span>
<span class="sd">        scaled by the square root of the group length so that each variable</span>
<span class="sd">        has the same effect on the penalty. This may not be appropriate for</span>
<span class="sd">        one-hot encoded features and ``scale_l2_by=None`` would be more</span>
<span class="sd">        appropriate for that case. ``scale_l2_by=None`` will also reproduce</span>
<span class="sd">        ElasticNet results when all features belong to one group.</span>

<span class="sd">    l1_ratio : float, default=1.0</span>
<span class="sd">        The elastic net mixing parameter, with ``0 &lt; l1_ratio &lt;= 1``.</span>
<span class="sd">        For ``l1_ratio = 0`` the penalty is an L2 penalty. (currently not</span>
<span class="sd">        supported) ``For l1_ratio = 1`` it is an L1 penalty. For</span>
<span class="sd">        ``0 &lt; l1_ratio &lt;1``, the penalty is a combination of L1 and L2.</span>

<span class="sd">    eps : float, default=1e-3</span>
<span class="sd">        Length of the path. ``eps=1e-3`` means that</span>
<span class="sd">        ``alpha_min / alpha_max = 1e-3``</span>

<span class="sd">    n_alphas : int, default=100</span>
<span class="sd">        Number of alphas along the regularization path</span>

<span class="sd">    fit_intercept : bool, default=True</span>
<span class="sd">        Whether to fit an intercept or not</span>

<span class="sd">    normalize : bool, default=False</span>
<span class="sd">        This parameter is ignored when ``fit_intercept`` is set to False.</span>
<span class="sd">        If True, the regressors X will be normalized before regression by</span>
<span class="sd">        subtracting the mean and dividing by the l2-norm.</span>
<span class="sd">        If you wish to standardize, please use</span>
<span class="sd">        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``</span>
<span class="sd">        on an estimator with ``normalize=False``.</span>

<span class="sd">    copy_X : bool, default=True</span>
<span class="sd">        If ``True``, X will be copied; else, it may be overwritten.</span>

<span class="sd">    model : class, default=SGL</span>
<span class="sd">        The estimator class that will be used to confirm that alpha_max sets</span>
<span class="sd">        all coef values to zero. The default value of ``model=SGL`` is</span>
<span class="sd">        appropriate for regression while ``model=LogisticSGL`` is appropriate</span>
<span class="sd">        for classification.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">l1_ratio</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_lasso_alpha_grid</span><span class="p">(</span>
            <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>
            <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
            <span class="n">Xy</span><span class="o">=</span><span class="n">Xy</span><span class="p">,</span>
            <span class="n">l1_ratio</span><span class="o">=</span><span class="n">l1_ratio</span><span class="p">,</span>
            <span class="n">fit_intercept</span><span class="o">=</span><span class="n">fit_intercept</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
            <span class="n">n_alphas</span><span class="o">=</span><span class="n">n_alphas</span><span class="p">,</span>
            <span class="n">normalize</span><span class="o">=</span><span class="n">normalize</span><span class="p">,</span>
            <span class="n">copy_X</span><span class="o">=</span><span class="n">copy_X</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">n_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">sparse_center</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">Xy</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">X_sparse</span> <span class="o">=</span> <span class="n">sparse</span><span class="o">.</span><span class="n">isspmatrix</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">sparse_center</span> <span class="o">=</span> <span class="n">X_sparse</span> <span class="ow">and</span> <span class="p">(</span><span class="n">fit_intercept</span> <span class="ow">or</span> <span class="n">normalize</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s2">&quot;csc&quot;</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="p">(</span><span class="n">copy_X</span> <span class="ow">and</span> <span class="n">fit_intercept</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">X_sparse</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">X_sparse</span><span class="p">:</span>
            <span class="c1"># X can be touched inplace thanks to the above line</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_preprocess_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="p">,</span> <span class="n">normalize</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">Xy</span> <span class="o">=</span> <span class="n">safe_sparse_dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">dense_output</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">sparse_center</span><span class="p">:</span>
            <span class="c1"># Workaround to find alpha_max for sparse matrices.</span>
            <span class="c1"># since we should not destroy the sparsity of such matrices.</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">X_offset</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">X_scale</span> <span class="o">=</span> <span class="n">_preprocess_data</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="p">,</span> <span class="n">normalize</span><span class="p">,</span> <span class="n">return_mean</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>
            <span class="n">mean_dot</span> <span class="o">=</span> <span class="n">X_offset</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">Xy</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">Xy</span> <span class="o">=</span> <span class="n">Xy</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">sparse_center</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">fit_intercept</span><span class="p">:</span>
            <span class="n">Xy</span> <span class="o">-=</span> <span class="n">mean_dot</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
            <span class="n">Xy</span> <span class="o">/=</span> <span class="n">X_scale</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

    <span class="n">groups</span> <span class="o">=</span> <span class="n">check_groups</span><span class="p">(</span><span class="n">groups</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">allow_overlap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">scale_l2_by</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;group_length&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;scale_l2_by must be &#39;group_length&#39; or None; &quot;</span> <span class="s2">&quot;got </span><span class="si">{0}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">scale_l2_by</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="c1"># When l1_ratio &lt; 1 (i.e. not the lasso), then for each group, the</span>
    <span class="c1"># smallest alpha for which coef_ = 0 minimizes the objective will be</span>
    <span class="c1"># achieved when</span>
    <span class="c1">#</span>
    <span class="c1"># || S(Xy / n_samples, l1_ratio * alpha) ||_2 == sqrt(p_l) * (1 - l1_ratio) * alpha</span>
    <span class="c1">#</span>
    <span class="c1"># where S() is the element-wise soft-thresholding operator and p_l is</span>
    <span class="c1"># the group size (or 1 if ``scale_l2_by is None``)</span>
    <span class="k">def</span> <span class="nf">beta_zero_root</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">group</span><span class="p">):</span>
        <span class="n">soft</span> <span class="o">=</span> <span class="n">_soft_threshold</span><span class="p">(</span><span class="n">Xy</span><span class="p">[</span><span class="n">group</span><span class="p">]</span> <span class="o">/</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="n">alpha</span><span class="p">)</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">group</span><span class="o">.</span><span class="n">size</span><span class="p">)</span> <span class="k">if</span> <span class="n">scale_l2_by</span> <span class="o">==</span> <span class="s2">&quot;group_length&quot;</span> <span class="k">else</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">soft</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">scale</span>

    <span class="c1"># We use the brentq method to find the root, which requires a bracket</span>
    <span class="c1"># within which to find the root. We know that ``beta_zero_root`` will</span>
    <span class="c1"># be positive when alpha=0. In order to ensure that the upper limit</span>
    <span class="c1"># brackets the root, we increase the upper limit until</span>
    <span class="c1"># ``beta_zero_root`` returns a negative number for all groups</span>
    <span class="k">def</span> <span class="nf">bracket_too_low</span><span class="p">(</span><span class="n">alpha</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">any</span><span class="p">([</span><span class="n">beta_zero_root</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">grp</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">grp</span> <span class="ow">in</span> <span class="n">groups</span><span class="p">])</span>

    <span class="n">upper_bracket_lim</span> <span class="o">=</span> <span class="mf">1e1</span>
    <span class="k">while</span> <span class="n">bracket_too_low</span><span class="p">(</span><span class="n">upper_bracket_lim</span><span class="p">):</span>
        <span class="n">upper_bracket_lim</span> <span class="o">*=</span> <span class="mi">10</span>

    <span class="n">min_alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">root_scalar</span><span class="p">(</span>
                <span class="n">partial</span><span class="p">(</span><span class="n">beta_zero_root</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">grp</span><span class="p">),</span>
                <span class="n">bracket</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">upper_bracket_lim</span><span class="p">],</span>
                <span class="n">method</span><span class="o">=</span><span class="s2">&quot;brentq&quot;</span><span class="p">,</span>
            <span class="p">)</span><span class="o">.</span><span class="n">root</span>
            <span class="k">for</span> <span class="n">grp</span> <span class="ow">in</span> <span class="n">groups</span>
        <span class="p">]</span>
    <span class="p">)</span>

    <span class="n">alpha_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">min_alphas</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.2</span>

    <span class="c1"># Test feature sparsity just to make sure we&#39;re on the right side of the root</span>
    <span class="k">while</span> <span class="p">(</span>
        <span class="n">model</span><span class="p">(</span>
            <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">alpha_max</span><span class="p">,</span>
            <span class="n">l1_ratio</span><span class="o">=</span><span class="n">l1_ratio</span><span class="p">,</span>
            <span class="n">fit_intercept</span><span class="o">=</span><span class="n">fit_intercept</span><span class="p">,</span>
            <span class="n">scale_l2_by</span><span class="o">=</span><span class="n">scale_l2_by</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="o">.</span><span class="n">chosen_features_</span><span class="o">.</span><span class="n">size</span>
        <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="p">):</span>
        <span class="n">alpha_max</span> <span class="o">*=</span> <span class="mf">1.2</span>

    <span class="k">if</span> <span class="n">alpha_max</span> <span class="o">&lt;=</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">resolution</span><span class="p">:</span>
        <span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_alphas</span><span class="p">)</span>
        <span class="n">alphas</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">resolution</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">alphas</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">alpha_max</span> <span class="o">*</span> <span class="n">eps</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">alpha_max</span><span class="p">),</span> <span class="n">num</span><span class="o">=</span><span class="n">n_alphas</span><span class="p">)[</span>
        <span class="p">::</span><span class="o">-</span><span class="mi">1</span>
    <span class="p">]</span>


<span class="nd">@registered</span>
<span class="k">def</span> <span class="nf">sgl_path</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span>
    <span class="n">y</span><span class="p">,</span>
    <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">groups</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">scale_l2_by</span><span class="o">=</span><span class="s2">&quot;group_length&quot;</span><span class="p">,</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
    <span class="n">n_alphas</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">Xy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">copy_X</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">return_n_iter</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">check_input</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">params</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute sparse group lasso path</span>

<span class="sd">    We use the previous solution as the initial guess for subsequent alpha values</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        Training data. Pass directly as Fortran-contiguous data to avoid</span>
<span class="sd">        unnecessary memory duplication.</span>

<span class="sd">    y : {array-like, sparse matrix} of shape (n_samples,)</span>
<span class="sd">        Target values.</span>

<span class="sd">    l1_ratio : float, default=0.5</span>
<span class="sd">        Number between 0 and 1 passed to SGL estimator (scaling between the</span>
<span class="sd">        group lasso and lasso penalties). ``l1_ratio=1`` corresponds to the</span>
<span class="sd">        Lasso.</span>

<span class="sd">    groups : list of numpy.ndarray</span>
<span class="sd">        list of arrays of non-overlapping indices for each group. For</span>
<span class="sd">        example, if nine features are grouped into equal contiguous groups of</span>
<span class="sd">        three, then groups would be ``[array([0, 1, 2]), array([3, 4, 5]),</span>
<span class="sd">        array([6, 7, 8])]``. If the feature matrix contains a bias or</span>
<span class="sd">        intercept feature, do not include it as a group. If None, all</span>
<span class="sd">        features will belong to one group.</span>

<span class="sd">    scale_l2_by : [&quot;group_length&quot;, None], default=&quot;group_length&quot;</span>
<span class="sd">        Scaling technique for the group-wise L2 penalty.</span>
<span class="sd">        By default, ``scale_l2_by=&quot;group_length`` and the L2 penalty is</span>
<span class="sd">        scaled by the square root of the group length so that each variable</span>
<span class="sd">        has the same effect on the penalty. This may not be appropriate for</span>
<span class="sd">        one-hot encoded features and ``scale_l2_by=None`` would be more</span>
<span class="sd">        appropriate for that case. ``scale_l2_by=None`` will also reproduce</span>
<span class="sd">        ElasticNet results when all features belong to one group.</span>

<span class="sd">    eps : float, default=1e-3</span>
<span class="sd">        Length of the path. ``eps=1e-3`` means that</span>
<span class="sd">        ``alpha_min / alpha_max = 1e-3``.</span>

<span class="sd">    n_alphas : int, default=100</span>
<span class="sd">        Number of alphas along the regularization path.</span>

<span class="sd">    alphas : ndarray, default=None</span>
<span class="sd">        List of alphas where to compute the models.</span>
<span class="sd">        If None alphas are set automatically.</span>

<span class="sd">    Xy : array-like of shape (n_features,), default=None</span>
<span class="sd">        Xy = np.dot(X.T, y) that can be precomputed.</span>

<span class="sd">    normalize : bool, default=False</span>
<span class="sd">        This parameter is ignored when ``fit_intercept`` is set to False.</span>
<span class="sd">        If True, the regressors X will be normalized before regression by</span>
<span class="sd">        subtracting the mean and dividing by the l2-norm.</span>
<span class="sd">        If you wish to standardize, please use</span>
<span class="sd">        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``</span>
<span class="sd">        on an estimator with ``normalize=False``.</span>

<span class="sd">    copy_X : bool, default=True</span>
<span class="sd">        If ``True``, X will be copied; else, it may be overwritten.</span>

<span class="sd">    verbose : bool or int, default=False</span>
<span class="sd">        Amount of verbosity.</span>

<span class="sd">    return_n_iter : bool, default=False</span>
<span class="sd">        Whether to return the number of iterations or not.</span>

<span class="sd">    check_input : bool, default=True</span>
<span class="sd">        Skip input validation checks, assuming there are handled by the</span>
<span class="sd">        caller when check_input=False.</span>

<span class="sd">    **params : kwargs</span>
<span class="sd">        Keyword arguments passed to the SGL estimator</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    alphas : ndarray of shape (n_alphas,)</span>
<span class="sd">        The alphas along the path where models are computed.</span>

<span class="sd">    coefs : ndarray of shape (n_features, n_alphas)</span>
<span class="sd">        Coefficients along the path.</span>

<span class="sd">    n_iters : list of int</span>
<span class="sd">        The number of iterations taken by the PGD solver to</span>
<span class="sd">        reach the specified tolerance for each alpha.</span>
<span class="sd">        (Is returned when ``return_n_iter`` is set to True).</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    SGL</span>
<span class="sd">    SGLCV</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># We expect X and y to be already Fortran ordered when bypassing</span>
    <span class="c1"># checks</span>
    <span class="k">if</span> <span class="n">check_input</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span>
            <span class="n">accept_sparse</span><span class="o">=</span><span class="s2">&quot;csc&quot;</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span>
            <span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">,</span>
            <span class="n">copy</span><span class="o">=</span><span class="n">copy_X</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span>
            <span class="n">y</span><span class="p">,</span>
            <span class="n">accept_sparse</span><span class="o">=</span><span class="s2">&quot;csc&quot;</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">type</span><span class="p">,</span>
            <span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">,</span>
            <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">ensure_2d</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">Xy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Xy should be a 1d contiguous array</span>
            <span class="n">Xy</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span>
                <span class="n">Xy</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">type</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ensure_2d</span><span class="o">=</span><span class="kc">False</span>
            <span class="p">)</span>
        <span class="n">groups</span> <span class="o">=</span> <span class="n">check_groups</span><span class="p">(</span><span class="n">groups</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">allow_overlap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">_</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="n">fit_intercept</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;fit_intercept&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">alphas</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">alphas</span> <span class="o">=</span> <span class="n">_alpha_grid</span><span class="p">(</span>
            <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>
            <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
            <span class="n">Xy</span><span class="o">=</span><span class="n">Xy</span><span class="p">,</span>
            <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
            <span class="n">scale_l2_by</span><span class="o">=</span><span class="n">scale_l2_by</span><span class="p">,</span>
            <span class="n">l1_ratio</span><span class="o">=</span><span class="n">l1_ratio</span><span class="p">,</span>
            <span class="n">fit_intercept</span><span class="o">=</span><span class="n">fit_intercept</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
            <span class="n">n_alphas</span><span class="o">=</span><span class="n">n_alphas</span><span class="p">,</span>
            <span class="n">normalize</span><span class="o">=</span><span class="n">normalize</span><span class="p">,</span>
            <span class="n">copy_X</span><span class="o">=</span><span class="n">copy_X</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">alphas</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># make sure alphas are properly ordered</span>

    <span class="n">n_alphas</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">alphas</span><span class="p">)</span>
    <span class="n">tol</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;tol&quot;</span><span class="p">,</span> <span class="mf">1e-7</span><span class="p">)</span>
    <span class="n">max_iter</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;max_iter&quot;</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span> <span class="s2">&quot;squared_loss&quot;</span><span class="p">)</span>
    <span class="n">n_iters</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n_alphas</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">coefs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_alphas</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">SGL</span><span class="p">(</span>
        <span class="n">l1_ratio</span><span class="o">=</span><span class="n">l1_ratio</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="n">alphas</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
        <span class="n">scale_l2_by</span><span class="o">=</span><span class="n">scale_l2_by</span><span class="p">,</span>
        <span class="n">fit_intercept</span><span class="o">=</span><span class="n">fit_intercept</span><span class="p">,</span>
        <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
        <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span>
        <span class="n">warm_start</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">suppress_solver_warnings</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">include_solver_trace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">verbose</span> <span class="ow">and</span> <span class="n">verbose</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">alpha_sequence</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Reg path&quot;</span><span class="p">,</span> <span class="n">total</span><span class="o">=</span><span class="n">n_alphas</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">alpha_sequence</span> <span class="o">=</span> <span class="n">alphas</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">alpha_sequence</span><span class="p">):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">)</span>

        <span class="n">coefs</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span>
        <span class="n">n_iters</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">n_iter_</span>

        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Path: </span><span class="si">%03i</span><span class="s2"> out of </span><span class="si">%03i</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">n_alphas</span><span class="p">))</span>

    <span class="c1"># TODO: Compute dual gaps here</span>
    <span class="n">dual_gaps</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">return_n_iter</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">alphas</span><span class="p">,</span> <span class="n">coefs</span><span class="p">,</span> <span class="n">dual_gaps</span><span class="p">,</span> <span class="n">n_iters</span>

    <span class="k">return</span> <span class="n">alphas</span><span class="p">,</span> <span class="n">coefs</span><span class="p">,</span> <span class="n">dual_gaps</span>


<div class="viewcode-block" id="SGLCV"><a class="viewcode-back" href="../../api.html#groupyr.SGLCV">[docs]</a><span class="nd">@registered</span>
<span class="k">class</span> <span class="nc">SGLCV</span><span class="p">(</span><span class="n">LinearModel</span><span class="p">,</span> <span class="n">RegressorMixin</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Class for iterative SGL model fitting along a regularization path</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    l1_ratio : float or list of float, default=1.0</span>
<span class="sd">        float between 0 and 1 passed to SGL (scaling between group lasso and</span>
<span class="sd">        lasso penalties). For ``l1_ratio = 0`` the penalty is the group lasso</span>
<span class="sd">        penalty. For ``l1_ratio = 1`` it is the lasso penalty. For ``0 &lt;</span>
<span class="sd">        l1_ratio &lt; 1``, the penalty is a combination of group lasso and</span>
<span class="sd">        lasso. This parameter can be a list, in which case the different</span>
<span class="sd">        values are tested by cross-validation and the one giving the best</span>
<span class="sd">        prediction score is used. Note that a good choice of list of values</span>
<span class="sd">        will depend on the problem. For problems where we expect strong</span>
<span class="sd">        overall sparsity and would like to encourage grouping, put more</span>
<span class="sd">        values close to 1 (i.e. Lasso). In contrast, if we expect strong</span>
<span class="sd">        group-wise sparsity, but only mild sparsity within groups, put more</span>
<span class="sd">        values close to 0 (i.e. group lasso).</span>

<span class="sd">    groups : list of numpy.ndarray</span>
<span class="sd">        list of arrays of non-overlapping indices for each group. For</span>
<span class="sd">        example, if nine features are grouped into equal contiguous groups of</span>
<span class="sd">        three, then groups would be ``[array([0, 1, 2]), array([3, 4, 5]),</span>
<span class="sd">        array([6, 7, 8])]``. If the feature matrix contains a bias or</span>
<span class="sd">        intercept feature, do not include it as a group. If None, all</span>
<span class="sd">        features will belong to one group. We set groups in ``__init__`` so</span>
<span class="sd">        that it can be reused in model selection and CV routines.</span>

<span class="sd">    scale_l2_by : [&quot;group_length&quot;, None], default=&quot;group_length&quot;</span>
<span class="sd">        Scaling technique for the group-wise L2 penalty.</span>
<span class="sd">        By default, ``scale_l2_by=&quot;group_length`` and the L2 penalty is</span>
<span class="sd">        scaled by the square root of the group length so that each variable</span>
<span class="sd">        has the same effect on the penalty. This may not be appropriate for</span>
<span class="sd">        one-hot encoded features and ``scale_l2_by=None`` would be more</span>
<span class="sd">        appropriate for that case. ``scale_l2_by=None`` will also reproduce</span>
<span class="sd">        ElasticNet results when all features belong to one group.</span>

<span class="sd">    eps : float, default=1e-3</span>
<span class="sd">        Length of the path. ``eps=1e-3`` means that</span>
<span class="sd">        ``alpha_min / alpha_max = 1e-3``.</span>

<span class="sd">    n_alphas : int, default=100</span>
<span class="sd">        Number of alphas along the regularization path, used for each l1_ratio.</span>

<span class="sd">    alphas : ndarray, default=None</span>
<span class="sd">        List of alphas where to compute the models.</span>
<span class="sd">        If None alphas are set automatically</span>

<span class="sd">    fit_intercept : bool, default=True</span>
<span class="sd">        whether to calculate the intercept for this model. If set</span>
<span class="sd">        to false, no intercept will be used in calculations</span>
<span class="sd">        (i.e. data is expected to be centered).</span>

<span class="sd">    normalize : bool, default=False</span>
<span class="sd">        This parameter is ignored when ``fit_intercept`` is set to False.</span>
<span class="sd">        If True, the regressors X will be normalized before regression by</span>
<span class="sd">        subtracting the mean and dividing by the l2-norm.</span>
<span class="sd">        If you wish to standardize, please use</span>
<span class="sd">        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``</span>
<span class="sd">        on an estimator with ``normalize=False``.</span>

<span class="sd">    max_iter : int, default=1000</span>
<span class="sd">        The maximum number of iterations</span>

<span class="sd">    tol : float, default=1e-7</span>
<span class="sd">        The tolerance for the SGL solver</span>

<span class="sd">    cv : int, cross-validation generator or iterable, default=None</span>
<span class="sd">        Determines the cross-validation splitting strategy.</span>
<span class="sd">        Possible inputs for cv are:</span>

<span class="sd">        - None, to use the default 5-fold cross-validation,</span>
<span class="sd">        - int, to specify the number of folds.</span>
<span class="sd">        - an sklearn `CV splitter &lt;https://scikit-learn.org/stable/glossary.html#term-cv-splitter&gt;`_,</span>
<span class="sd">        - An iterable yielding (train, test) splits as arrays of indices.</span>

<span class="sd">        For int/None inputs, :class:`KFold` is used.</span>

<span class="sd">        Refer to the scikit-learn User Guide for the various</span>
<span class="sd">        cross-validation strategies that can be used here.</span>

<span class="sd">    copy_X : bool, default=True</span>
<span class="sd">        If ``True``, X will be copied; else, it may be overwritten.</span>

<span class="sd">    verbose : bool or int, default=0</span>
<span class="sd">        Amount of verbosity.</span>

<span class="sd">    n_jobs : int, default=None</span>
<span class="sd">        Number of CPUs to use during the cross validation.</span>
<span class="sd">        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.</span>
<span class="sd">        ``-1`` means using all processors.</span>

<span class="sd">    random_state : int, RandomState instance, default=None</span>
<span class="sd">        The seed of the pseudo random number generator that selects a random</span>
<span class="sd">        feature to update. Used when ``selection`` == &#39;random&#39;.</span>
<span class="sd">        Pass an int for reproducible output across multiple function calls.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    alpha_ : float</span>
<span class="sd">        The amount of penalization chosen by cross validation</span>

<span class="sd">    l1_ratio_ : float</span>
<span class="sd">        The compromise between l1 and l2 penalization chosen by</span>
<span class="sd">        cross validation</span>

<span class="sd">    coef_ : ndarray of shape (n_features,) or (n_targets, n_features)</span>
<span class="sd">        Parameter vector (w in the cost function formula),</span>

<span class="sd">    intercept_ : float or ndarray of shape (n_targets, n_features)</span>
<span class="sd">        Independent term in the decision function.</span>

<span class="sd">    mse_path_ : ndarray of shape (n_l1_ratio, n_alpha, n_folds)</span>
<span class="sd">        Mean square error for the test set on each fold, varying l1_ratio and</span>
<span class="sd">        alpha.</span>

<span class="sd">    alphas_ : ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)</span>
<span class="sd">        The grid of alphas used for fitting, for each l1_ratio.</span>

<span class="sd">    n_iter_ : int</span>
<span class="sd">        number of iterations run by the proximal gradient descent solver to</span>
<span class="sd">        reach the specified tolerance for the optimal alpha.</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    sgl_path</span>
<span class="sd">    SGL</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="n">groups</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">scale_l2_by</span><span class="o">=</span><span class="s2">&quot;group_length&quot;</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
        <span class="n">n_alphas</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
        <span class="n">tol</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span>
        <span class="n">copy_X</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">cv</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span> <span class="o">=</span> <span class="n">l1_ratio</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">=</span> <span class="n">groups</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale_l2_by</span> <span class="o">=</span> <span class="n">scale_l2_by</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_alphas</span> <span class="o">=</span> <span class="n">n_alphas</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alphas</span> <span class="o">=</span> <span class="n">alphas</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span> <span class="o">=</span> <span class="n">fit_intercept</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">normalize</span> <span class="o">=</span> <span class="n">normalize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">=</span> <span class="n">max_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">copy_X</span> <span class="o">=</span> <span class="n">copy_X</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cv</span> <span class="o">=</span> <span class="n">cv</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="n">n_jobs</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit sparse group lasso linear model</span>

<span class="sd">        Fit is on grid of alphas and best alpha estimated by cross-validation.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            Training data. Pass directly as Fortran-contiguous data</span>
<span class="sd">            to avoid unnecessary memory duplication. If y is mono-output,</span>
<span class="sd">            X can be sparse.</span>

<span class="sd">        y : array-like of shape (n_samples,) or (n_samples, n_targets)</span>
<span class="sd">            Target values</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># This makes sure that there is no duplication in memory.</span>
        <span class="c1"># Dealing right with copy_X is important in the following:</span>
        <span class="c1"># Multiple functions touch X and subsamples of X and can induce a</span>
        <span class="c1"># lot of duplication of memory</span>
        <span class="n">copy_X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">copy_X</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span>

        <span class="n">check_y_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
            <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="n">ensure_2d</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="ow">or</span> <span class="n">sparse</span><span class="o">.</span><span class="n">isspmatrix</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="c1"># Keep a reference to X</span>
            <span class="n">reference_to_old_X</span> <span class="o">=</span> <span class="n">X</span>
            <span class="c1"># Let us not impose fortran ordering so far: it is</span>
            <span class="c1"># not useful for the cross-validation loop and will be done</span>
            <span class="c1"># by the model fitting itself</span>

            <span class="c1"># Need to validate separately here.</span>
            <span class="c1"># We can&#39;t pass multi_ouput=True because that would allow y to be</span>
            <span class="c1"># csr. We also want to allow y to be 64 or 32 but check_X_y only</span>
            <span class="c1"># allows to convert for 64.</span>
            <span class="n">check_X_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
                <span class="n">accept_sparse</span><span class="o">=</span><span class="s2">&quot;csc&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span>
            <span class="p">)</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validate_separately</span><span class="o">=</span><span class="p">(</span><span class="n">check_X_params</span><span class="p">,</span> <span class="n">check_y_params</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">sparse</span><span class="o">.</span><span class="n">isspmatrix</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">reference_to_old_X</span><span class="p">,</span> <span class="s2">&quot;data&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">may_share_memory</span><span class="p">(</span>
                    <span class="n">reference_to_old_X</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">data</span>
                <span class="p">):</span>
                    <span class="c1"># X is a sparse matrix and has been copied</span>
                    <span class="n">copy_X</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">may_share_memory</span><span class="p">(</span><span class="n">reference_to_old_X</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
                <span class="c1"># X has been copied</span>
                <span class="n">copy_X</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">del</span> <span class="n">reference_to_old_X</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Need to validate separately here.</span>
            <span class="c1"># We can&#39;t pass multi_ouput=True because that would allow y to be</span>
            <span class="c1"># csr. We also want to allow y to be 64 or 32 but check_X_y only</span>
            <span class="c1"># allows to convert for 64.</span>
            <span class="n">check_X_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
                <span class="n">accept_sparse</span><span class="o">=</span><span class="s2">&quot;csc&quot;</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span>
                <span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">,</span>
                <span class="n">copy</span><span class="o">=</span><span class="n">copy_X</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validate_separately</span><span class="o">=</span><span class="p">(</span><span class="n">check_X_params</span><span class="p">,</span> <span class="n">check_y_params</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">copy_X</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;y has 0 samples: </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">y</span><span class="p">)</span>

        <span class="n">model</span> <span class="o">=</span> <span class="n">SGL</span><span class="p">()</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">column_or_1d</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">warn</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;X and y have inconsistent dimensions (</span><span class="si">%d</span><span class="s2"> != </span><span class="si">%d</span><span class="s2">)&quot;</span>
                <span class="o">%</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="p">)</span>

        <span class="n">groups</span> <span class="o">=</span> <span class="n">check_groups</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">allow_overlap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># All SGLCV parameters except &quot;cv&quot; and &quot;n_jobs&quot; are acceptable</span>
        <span class="n">path_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
        <span class="n">path_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cv&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">path_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;n_jobs&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="n">l1_ratios</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">path_params</span><span class="p">[</span><span class="s2">&quot;l1_ratio&quot;</span><span class="p">])</span>
        <span class="c1"># For the first path, we need to set l1_ratio</span>
        <span class="n">path_params</span><span class="p">[</span><span class="s2">&quot;l1_ratio&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">l1_ratios</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">alphas</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alphas</span>
        <span class="n">n_l1_ratio</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">l1_ratios</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">alphas</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">alphas</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">_alpha_grid</span><span class="p">(</span>
                    <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>
                    <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                    <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
                    <span class="n">scale_l2_by</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">scale_l2_by</span><span class="p">,</span>
                    <span class="n">l1_ratio</span><span class="o">=</span><span class="n">l1_ratio</span><span class="p">,</span>
                    <span class="n">fit_intercept</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">,</span>
                    <span class="n">eps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span>
                    <span class="n">n_alphas</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_alphas</span><span class="p">,</span>
                    <span class="n">normalize</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">normalize</span><span class="p">,</span>
                    <span class="n">copy_X</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">copy_X</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">l1_ratio</span> <span class="ow">in</span> <span class="n">l1_ratios</span>
            <span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Making sure alphas is properly ordered.</span>
            <span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">alphas</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="n">n_l1_ratio</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="c1"># We want n_alphas to be the number of alphas used for each l1_ratio.</span>
        <span class="n">n_alphas</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">alphas</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">path_params</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;n_alphas&quot;</span><span class="p">:</span> <span class="n">n_alphas</span><span class="p">})</span>

        <span class="n">path_params</span><span class="p">[</span><span class="s2">&quot;copy_X&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">copy_X</span>
        <span class="c1"># We are not computing in parallel, we can modify X</span>
        <span class="c1"># inplace in the folds</span>
        <span class="k">if</span> <span class="n">effective_n_jobs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">path_params</span><span class="p">[</span><span class="s2">&quot;copy_X&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># &quot;precompute&quot; has no effect but it is expected by _path_residuals</span>
        <span class="n">path_params</span><span class="p">[</span><span class="s2">&quot;precompute&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">path_params</span><span class="p">[</span><span class="s2">&quot;verbose&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="c1"># init cross-validation generator</span>
        <span class="n">cv</span> <span class="o">=</span> <span class="n">check_cv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cv</span><span class="p">)</span>

        <span class="c1"># Compute path for all folds and compute MSE to get the best alpha</span>
        <span class="n">folds</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
        <span class="n">best_mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>

        <span class="c1"># We do a double for loop folded in one, in order to be able to</span>
        <span class="c1"># iterate in parallel on l1_ratio and folds</span>
        <span class="n">jobs</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">delayed</span><span class="p">(</span><span class="n">_path_residuals</span><span class="p">)(</span>
                <span class="n">X</span><span class="p">,</span>
                <span class="n">y</span><span class="p">,</span>
                <span class="n">train</span><span class="p">,</span>
                <span class="n">test</span><span class="p">,</span>
                <span class="n">sgl_path</span><span class="p">,</span>
                <span class="n">path_params</span><span class="p">,</span>
                <span class="n">alphas</span><span class="o">=</span><span class="n">this_alphas</span><span class="p">,</span>
                <span class="n">l1_ratio</span><span class="o">=</span><span class="n">this_l1_ratio</span><span class="p">,</span>
                <span class="n">X_order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">type</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">this_l1_ratio</span><span class="p">,</span> <span class="n">this_alphas</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">l1_ratios</span><span class="p">,</span> <span class="n">alphas</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">folds</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">parallel_verbosity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">-</span> <span class="mi">2</span>
            <span class="k">if</span> <span class="n">parallel_verbosity</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">parallel_verbosity</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">parallel_verbosity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span>

        <span class="n">mse_paths</span> <span class="o">=</span> <span class="n">ProgressParallel</span><span class="p">(</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">parallel_verbosity</span><span class="p">,</span>
            <span class="n">use_tqdm</span><span class="o">=</span><span class="nb">bool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">),</span>
            <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;L1 ratios * CV folds&quot;</span><span class="p">,</span>
            <span class="n">total</span><span class="o">=</span><span class="n">n_l1_ratio</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">folds</span><span class="p">),</span>
            <span class="o">**</span><span class="n">_joblib_parallel_args</span><span class="p">(</span><span class="n">prefer</span><span class="o">=</span><span class="s2">&quot;threads&quot;</span><span class="p">),</span>
        <span class="p">)(</span><span class="n">jobs</span><span class="p">)</span>

        <span class="n">mse_paths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">mse_paths</span><span class="p">,</span> <span class="p">(</span><span class="n">n_l1_ratio</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">folds</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">mean_mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">mse_paths</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mse_path_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">rollaxis</span><span class="p">(</span><span class="n">mse_paths</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="k">for</span> <span class="n">l1_ratio</span><span class="p">,</span> <span class="n">l1_alphas</span><span class="p">,</span> <span class="n">mse_alphas</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">l1_ratios</span><span class="p">,</span> <span class="n">alphas</span><span class="p">,</span> <span class="n">mean_mse</span><span class="p">):</span>
            <span class="n">i_best_alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">mse_alphas</span><span class="p">)</span>
            <span class="n">this_best_mse</span> <span class="o">=</span> <span class="n">mse_alphas</span><span class="p">[</span><span class="n">i_best_alpha</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">this_best_mse</span> <span class="o">&lt;</span> <span class="n">best_mse</span><span class="p">:</span>
                <span class="n">best_alpha</span> <span class="o">=</span> <span class="n">l1_alphas</span><span class="p">[</span><span class="n">i_best_alpha</span><span class="p">]</span>
                <span class="n">best_l1_ratio</span> <span class="o">=</span> <span class="n">l1_ratio</span>
                <span class="n">best_mse</span> <span class="o">=</span> <span class="n">this_best_mse</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio_</span> <span class="o">=</span> <span class="n">best_l1_ratio</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha_</span> <span class="o">=</span> <span class="n">best_alpha</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">alphas</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">alphas_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">alphas</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">n_l1_ratio</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">alphas_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alphas_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># Remove duplicate alphas in case alphas is provided.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">alphas_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">alphas</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="c1"># Refit the model with the parameters selected</span>
        <span class="n">common_params</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">name</span><span class="p">:</span> <span class="n">value</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
        <span class="p">}</span>

        <span class="n">model</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">common_params</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">best_alpha</span>
        <span class="n">model</span><span class="o">.</span><span class="n">l1_ratio</span> <span class="o">=</span> <span class="n">best_l1_ratio</span>
        <span class="n">model</span><span class="o">.</span><span class="n">copy_X</span> <span class="o">=</span> <span class="n">copy_X</span>

        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">n_iter_</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_fitted_</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">chosen_features_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;An index array of chosen features&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">sparsity_mask_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;A boolean array indicating which features survived regularization&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">!=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">like_nonzero_mask_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;A boolean array indicating which features are zero or close to zero</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        rtol : float</span>
<span class="sd">            Relative tolerance. Any features that are larger in magnitude</span>
<span class="sd">            than ``rtol`` times the mean coefficient value are considered</span>
<span class="sd">            nonzero-like.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">mean_abs_coef</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">rtol</span> <span class="o">*</span> <span class="n">mean_abs_coef</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">chosen_groups_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;A set of the group IDs that survived regularization&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">group_mask</span> <span class="o">=</span> <span class="p">[</span>
                <span class="nb">bool</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">grp</span><span class="p">)</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">chosen_features_</span><span class="p">)))</span>
                <span class="k">for</span> <span class="n">grp</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span>
            <span class="p">]</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">group_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">chosen_features_</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Remove columns corresponding to zeroed-out coefficients&quot;&quot;&quot;</span>
        <span class="c1"># Check is fit had been called</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;is_fitted_&quot;</span><span class="p">)</span>

        <span class="c1"># Input validation</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Check that the input is of the same shape as the one passed</span>
        <span class="c1"># during fit.</span>
        <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Shape of input is different from what was seen in `fit`&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">X</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparsity_mask_</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_more_tags</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># pylint: disable=no-self-use</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;multioutput&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;requires_y&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Adam Richie-Halford

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>