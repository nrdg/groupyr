

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>groupyr.sgl &mdash; groupyr 0.2.1 documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/groupyr.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/panels-bootstrap.5fd3999ee7762ccc51105388f4a9d115.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery-rendered-html.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../_static/groupyr-logo.ico"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/js/copybutton.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> groupyr
          

          
            
            <img src="../../_static/groupyr-logo.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.2.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../index.html">Home</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../auto_examples/index.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_help.html">Getting help using <em>groupyr</em></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing.html">Contributing to <em>groupyr</em></a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/richford/groupyr">Groupyr on GitHub</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">groupyr</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>groupyr.sgl</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for groupyr.sgl</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Create regression estimators based on the sparse group lasso.&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">contextlib</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">delayed</span><span class="p">,</span> <span class="n">effective_n_jobs</span><span class="p">,</span> <span class="n">Parallel</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">sparse</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">root_scalar</span>
<span class="kn">from</span> <span class="nn">skopt</span> <span class="kn">import</span> <span class="n">BayesSearchCV</span>
<span class="kn">from</span> <span class="nn">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">RegressorMixin</span><span class="p">,</span> <span class="n">TransformerMixin</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model._base</span> <span class="kn">import</span> <span class="n">LinearModel</span><span class="p">,</span> <span class="n">_preprocess_data</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model._coordinate_descent</span> <span class="kn">import</span> <span class="n">_alpha_grid</span> <span class="k">as</span> <span class="n">_lasso_alpha_grid</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">get_scorer</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">check_cv</span>
<span class="kn">from</span> <span class="nn">sklearn.utils.extmath</span> <span class="kn">import</span> <span class="n">safe_sparse_dot</span>
<span class="kn">from</span> <span class="nn">sklearn.utils.fixes</span> <span class="kn">import</span> <span class="n">_joblib_parallel_args</span>
<span class="kn">from</span> <span class="nn">sklearn.utils.validation</span> <span class="kn">import</span> <span class="n">check_array</span><span class="p">,</span> <span class="n">check_is_fitted</span><span class="p">,</span> <span class="n">column_or_1d</span>

<span class="kn">from</span> <span class="nn">._base</span> <span class="kn">import</span> <span class="n">SGLBaseEstimator</span>
<span class="kn">from</span> <span class="nn">._prox</span> <span class="kn">import</span> <span class="n">_soft_threshold</span>
<span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="n">check_groups</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;SGL&quot;</span><span class="p">,</span> <span class="s2">&quot;sgl_path&quot;</span><span class="p">,</span> <span class="s2">&quot;SGLCV&quot;</span><span class="p">]</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<div class="viewcode-block" id="SGL"><a class="viewcode-back" href="../../api.html#groupyr.SGL">[docs]</a><span class="k">class</span> <span class="nc">SGL</span><span class="p">(</span><span class="n">SGLBaseEstimator</span><span class="p">,</span> <span class="n">RegressorMixin</span><span class="p">,</span> <span class="n">LinearModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;An sklearn compatible sparse group lasso regressor.</span>

<span class="sd">    This solves the sparse group lasso [1]_ problem for a feature matrix</span>
<span class="sd">    partitioned into groups using the proximal gradient descent (PGD)</span>
<span class="sd">    algorithm.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    l1_ratio : float, default=1.0</span>
<span class="sd">        Hyper-parameter : Combination between group lasso and lasso. l1_ratio=0</span>
<span class="sd">        gives the group lasso and l1_ratio=1 gives the lasso.</span>

<span class="sd">    alpha : float, default=1.0</span>
<span class="sd">        Hyper-parameter : overall regularization strength.</span>

<span class="sd">    groups : list of numpy.ndarray</span>
<span class="sd">        list of arrays of non-overlapping indices for each group. For</span>
<span class="sd">        example, if nine features are grouped into equal contiguous groups of</span>
<span class="sd">        three, then groups would be ``[array([0, 1, 2]), array([3, 4, 5]),</span>
<span class="sd">        array([6, 7, 8])]``. If the feature matrix contains a bias or</span>
<span class="sd">        intercept feature, do not include it as a group. If None, all</span>
<span class="sd">        features will belong to one group. We set groups in ``__init__`` so</span>
<span class="sd">        that it can be reused in model selection and CV routines.</span>

<span class="sd">    scale_l2_by : [&quot;group_length&quot;, None], default=&quot;group_length&quot;</span>
<span class="sd">        Scaling technique for the group-wise L2 penalty.</span>
<span class="sd">        By default, ``scale_l2_by=&quot;group_length`` and the L2 penalty is</span>
<span class="sd">        scaled by the square root of the group length so that each variable</span>
<span class="sd">        has the same effect on the penalty. This may not be appropriate for</span>
<span class="sd">        one-hot encoded features and ``scale_l2_by=None`` would be more</span>
<span class="sd">        appropriate for that case. ``scale_l2_by=None`` will also reproduce</span>
<span class="sd">        ElasticNet results when all features belong to one group.</span>

<span class="sd">    fit_intercept : bool, default=True</span>
<span class="sd">        Specifies if a constant (a.k.a. bias or intercept) should be</span>
<span class="sd">        added to the linear predictor (X @ coef + intercept).</span>

<span class="sd">    max_iter : int, default=1000</span>
<span class="sd">        Maximum number of iterations for PGD solver.</span>

<span class="sd">    tol : float, default=1e-7</span>
<span class="sd">        Stopping criterion. Convergence tolerance for the ``copt`` proximal gradient solver</span>

<span class="sd">    warm_start : bool, default=False</span>
<span class="sd">        If set to ``True``, reuse the solution of the previous call to ``fit``</span>
<span class="sd">        as initialization for ``coef_`` and ``intercept_``.</span>

<span class="sd">    verbose : int, default=0</span>
<span class="sd">        Verbosity flag for PGD solver. Any positive integer will produce</span>
<span class="sd">        verbose output</span>

<span class="sd">    suppress_solver_warnings : bool, default=True</span>
<span class="sd">        If True, suppress convergence warnings from PGD solver.</span>
<span class="sd">        This is useful for hyperparameter tuning when some combinations</span>
<span class="sd">        of hyperparameters may not converge.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    coef_ : array of shape (n_features,)</span>
<span class="sd">        Estimated coefficients for the linear predictor (`X @ coef_ +</span>
<span class="sd">        intercept_`).</span>

<span class="sd">    intercept_ : float</span>
<span class="sd">        Intercept (a.k.a. bias) added to linear predictor.</span>

<span class="sd">    n_iter_ : int</span>
<span class="sd">        Actual number of iterations used in the solver.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1]  Noah Simon, Jerome Friedman, Trevor Hastie &amp; Robert Tibshirani,</span>
<span class="sd">        &quot;A Sparse-Group Lasso,&quot; Journal of Computational and Graphical</span>
<span class="sd">        Statistics, vol. 22:2, pp. 231-245, 2012</span>
<span class="sd">        DOI: 10.1080/10618600.2012.681250</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;squared_loss&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit a linear model using the sparse group lasso.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>
<span class="sd">            The training input samples.</span>

<span class="sd">        y : array-like, shape (n_samples,)</span>
<span class="sd">            The target values (class labels in classification, real numbers in</span>
<span class="sd">            regression).</span>

<span class="sd">        loss : [&quot;squared_loss&quot;, &quot;huber&quot;]</span>
<span class="sd">            The type of loss function to use in the PGD solver.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Returns self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict targets for test vectors in ``X``.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>
<span class="sd">            The training input samples.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : ndarray, shape (n_samples,)</span>
<span class="sd">            Returns an array of ones.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;is_fitted_&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">safe_sparse_dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dense_output</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span>

    <span class="k">def</span> <span class="nf">_more_tags</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># pylint: disable=no-self-use</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;requires_y&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span></div>


<span class="k">def</span> <span class="nf">_alpha_grid</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span>
    <span class="n">y</span><span class="p">,</span>
    <span class="n">Xy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">groups</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">scale_l2_by</span><span class="o">=</span><span class="s2">&quot;group_length&quot;</span><span class="p">,</span>
    <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
    <span class="n">n_alphas</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">copy_X</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">SGL</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the grid of alpha values for elastic net parameter search.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        Training data. Pass directly as Fortran-contiguous data to avoid</span>
<span class="sd">        unnecessary memory duplication</span>

<span class="sd">    y : ndarray of shape (n_samples,)</span>
<span class="sd">        Target values</span>

<span class="sd">    Xy : array-like of shape (n_features,), default=None</span>
<span class="sd">        Xy = np.dot(X.T, y) that can be precomputed. If supplying ``Xy``,</span>
<span class="sd">        prevent train/test leakage by ensuring the ``Xy`` is precomputed</span>
<span class="sd">        using only training data.</span>

<span class="sd">    groups : list of numpy.ndarray</span>
<span class="sd">        list of arrays of non-overlapping indices for each group. For</span>
<span class="sd">        example, if nine features are grouped into equal contiguous groups of</span>
<span class="sd">        three, then groups would be ``[array([0, 1, 2]), array([3, 4, 5]),</span>
<span class="sd">        array([6, 7, 8])]``. If the feature matrix contains a bias or</span>
<span class="sd">        intercept feature, do not include it as a group. If None, all</span>
<span class="sd">        features will belong to one group.</span>

<span class="sd">    scale_l2_by : [&quot;group_length&quot;, None], default=&quot;group_length&quot;</span>
<span class="sd">        Scaling technique for the group-wise L2 penalty.</span>
<span class="sd">        By default, ``scale_l2_by=&quot;group_length`` and the L2 penalty is</span>
<span class="sd">        scaled by the square root of the group length so that each variable</span>
<span class="sd">        has the same effect on the penalty. This may not be appropriate for</span>
<span class="sd">        one-hot encoded features and ``scale_l2_by=None`` would be more</span>
<span class="sd">        appropriate for that case. ``scale_l2_by=None`` will also reproduce</span>
<span class="sd">        ElasticNet results when all features belong to one group.</span>

<span class="sd">    l1_ratio : float, default=1.0</span>
<span class="sd">        The elastic net mixing parameter, with ``0 &lt; l1_ratio &lt;= 1``.</span>
<span class="sd">        For ``l1_ratio = 0`` the penalty is an L2 penalty. (currently not</span>
<span class="sd">        supported) ``For l1_ratio = 1`` it is an L1 penalty. For</span>
<span class="sd">        ``0 &lt; l1_ratio &lt;1``, the penalty is a combination of L1 and L2.</span>

<span class="sd">    eps : float, default=1e-3</span>
<span class="sd">        Length of the path. ``eps=1e-3`` means that</span>
<span class="sd">        ``alpha_min / alpha_max = 1e-3``</span>

<span class="sd">    n_alphas : int, default=100</span>
<span class="sd">        Number of alphas along the regularization path</span>

<span class="sd">    fit_intercept : bool, default=True</span>
<span class="sd">        Whether to fit an intercept or not</span>

<span class="sd">    normalize : bool, default=False</span>
<span class="sd">        This parameter is ignored when ``fit_intercept`` is set to False.</span>
<span class="sd">        If True, the regressors X will be normalized before regression by</span>
<span class="sd">        subtracting the mean and dividing by the l2-norm.</span>
<span class="sd">        If you wish to standardize, please use</span>
<span class="sd">        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``</span>
<span class="sd">        on an estimator with ``normalize=False``.</span>

<span class="sd">    copy_X : bool, default=True</span>
<span class="sd">        If ``True``, X will be copied; else, it may be overwritten.</span>

<span class="sd">    model : class, default=SGL</span>
<span class="sd">        The estimator class that will be used to confirm that alpha_max sets</span>
<span class="sd">        all coef values to zero. The default value of ``model=SGL`` is</span>
<span class="sd">        appropriate for regression while ``model=LogisticSGL`` is appropriate</span>
<span class="sd">        for classification.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">l1_ratio</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_lasso_alpha_grid</span><span class="p">(</span>
            <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>
            <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
            <span class="n">Xy</span><span class="o">=</span><span class="n">Xy</span><span class="p">,</span>
            <span class="n">l1_ratio</span><span class="o">=</span><span class="n">l1_ratio</span><span class="p">,</span>
            <span class="n">fit_intercept</span><span class="o">=</span><span class="n">fit_intercept</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
            <span class="n">n_alphas</span><span class="o">=</span><span class="n">n_alphas</span><span class="p">,</span>
            <span class="n">normalize</span><span class="o">=</span><span class="n">normalize</span><span class="p">,</span>
            <span class="n">copy_X</span><span class="o">=</span><span class="n">copy_X</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">n_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">Xy</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="p">(</span><span class="n">copy_X</span> <span class="ow">and</span> <span class="n">fit_intercept</span><span class="p">))</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_preprocess_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="p">,</span> <span class="n">normalize</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">Xy</span> <span class="o">=</span> <span class="n">safe_sparse_dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">dense_output</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">Xy</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">Xy</span> <span class="o">=</span> <span class="n">Xy</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

    <span class="n">groups</span> <span class="o">=</span> <span class="n">check_groups</span><span class="p">(</span><span class="n">groups</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">allow_overlap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">scale_l2_by</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;group_length&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;scale_l2_by must be &#39;group_length&#39; or None; &quot;</span> <span class="s2">&quot;got </span><span class="si">{0}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">scale_l2_by</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="c1"># When l1_ratio &lt; 1 (i.e. not the lasso), then for each group, the</span>
    <span class="c1"># smallest alpha for which coef_ = 0 minimizes the objective will be</span>
    <span class="c1"># achieved when</span>
    <span class="c1">#</span>
    <span class="c1"># || S(Xy / n_samples, l1_ratio * alpha) ||_2 == sqrt(p_l) * (1 - l1_ratio) * alpha</span>
    <span class="c1">#</span>
    <span class="c1"># where S() is the element-wise soft-thresholding operator and p_l is</span>
    <span class="c1"># the group size (or 1 if ``scale_l2_by is None``)</span>
    <span class="k">def</span> <span class="nf">beta_zero_root</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">group</span><span class="p">):</span>
        <span class="n">soft</span> <span class="o">=</span> <span class="n">_soft_threshold</span><span class="p">(</span><span class="n">Xy</span><span class="p">[</span><span class="n">group</span><span class="p">]</span> <span class="o">/</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="n">alpha</span><span class="p">)</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">group</span><span class="o">.</span><span class="n">size</span><span class="p">)</span> <span class="k">if</span> <span class="n">scale_l2_by</span> <span class="o">==</span> <span class="s2">&quot;group_length&quot;</span> <span class="k">else</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">soft</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">scale</span>

    <span class="c1"># We use the brentq method to find the root, which requires a bracket</span>
    <span class="c1"># within which to find the root. We know that ``beta_zero_root`` will</span>
    <span class="c1"># be positive when alpha=0. In order to ensure that the upper limit</span>
    <span class="c1"># brackets the root, we increase the upper limit until</span>
    <span class="c1"># ``beta_zero_root`` returns a negative number for all groups</span>
    <span class="k">def</span> <span class="nf">bracket_too_low</span><span class="p">(</span><span class="n">alpha</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">any</span><span class="p">([</span><span class="n">beta_zero_root</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">grp</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">grp</span> <span class="ow">in</span> <span class="n">groups</span><span class="p">])</span>

    <span class="n">upper_bracket_lim</span> <span class="o">=</span> <span class="mf">1e1</span>
    <span class="k">while</span> <span class="n">bracket_too_low</span><span class="p">(</span><span class="n">upper_bracket_lim</span><span class="p">):</span>
        <span class="n">upper_bracket_lim</span> <span class="o">*=</span> <span class="mi">10</span>

    <span class="n">min_alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">root_scalar</span><span class="p">(</span>
                <span class="n">partial</span><span class="p">(</span><span class="n">beta_zero_root</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">grp</span><span class="p">),</span>
                <span class="n">bracket</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">upper_bracket_lim</span><span class="p">],</span>
                <span class="n">method</span><span class="o">=</span><span class="s2">&quot;brentq&quot;</span><span class="p">,</span>
            <span class="p">)</span><span class="o">.</span><span class="n">root</span>
            <span class="k">for</span> <span class="n">grp</span> <span class="ow">in</span> <span class="n">groups</span>
        <span class="p">]</span>
    <span class="p">)</span>

    <span class="n">alpha_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">min_alphas</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.2</span>

    <span class="c1"># Test feature sparsity just to make sure we&#39;re on the right side of the root</span>
    <span class="k">while</span> <span class="p">(</span>  <span class="c1"># pragma: no cover</span>
        <span class="n">model</span><span class="p">(</span>
            <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">alpha_max</span><span class="p">,</span>
            <span class="n">l1_ratio</span><span class="o">=</span><span class="n">l1_ratio</span><span class="p">,</span>
            <span class="n">fit_intercept</span><span class="o">=</span><span class="n">fit_intercept</span><span class="p">,</span>
            <span class="n">scale_l2_by</span><span class="o">=</span><span class="n">scale_l2_by</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="o">.</span><span class="n">chosen_features_</span><span class="o">.</span><span class="n">size</span>
        <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="p">):</span>
        <span class="n">alpha_max</span> <span class="o">*=</span> <span class="mf">1.2</span>  <span class="c1"># pragma: no cover</span>

    <span class="k">if</span> <span class="n">alpha_max</span> <span class="o">&lt;=</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">resolution</span><span class="p">:</span>
        <span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_alphas</span><span class="p">)</span>
        <span class="n">alphas</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">resolution</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">alphas</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">alpha_max</span> <span class="o">*</span> <span class="n">eps</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">alpha_max</span><span class="p">),</span> <span class="n">num</span><span class="o">=</span><span class="n">n_alphas</span><span class="p">)[</span>
        <span class="p">::</span><span class="o">-</span><span class="mi">1</span>
    <span class="p">]</span>


<span class="k">def</span> <span class="nf">sgl_path</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span>
    <span class="n">y</span><span class="p">,</span>
    <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">groups</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">scale_l2_by</span><span class="o">=</span><span class="s2">&quot;group_length&quot;</span><span class="p">,</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
    <span class="n">n_alphas</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">Xy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">copy_X</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">return_n_iter</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">check_input</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">params</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute sparse group lasso path.</span>

<span class="sd">    We use the previous solution as the initial guess for subsequent alpha values</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        Training data. Pass directly as Fortran-contiguous data to avoid</span>
<span class="sd">        unnecessary memory duplication.</span>

<span class="sd">    y : {array-like, sparse matrix} of shape (n_samples,)</span>
<span class="sd">        Target values.</span>

<span class="sd">    l1_ratio : float, default=0.5</span>
<span class="sd">        Number between 0 and 1 passed to SGL estimator (scaling between the</span>
<span class="sd">        group lasso and lasso penalties). ``l1_ratio=1`` corresponds to the</span>
<span class="sd">        Lasso.</span>

<span class="sd">    groups : list of numpy.ndarray</span>
<span class="sd">        list of arrays of non-overlapping indices for each group. For</span>
<span class="sd">        example, if nine features are grouped into equal contiguous groups of</span>
<span class="sd">        three, then groups would be ``[array([0, 1, 2]), array([3, 4, 5]),</span>
<span class="sd">        array([6, 7, 8])]``. If the feature matrix contains a bias or</span>
<span class="sd">        intercept feature, do not include it as a group. If None, all</span>
<span class="sd">        features will belong to one group.</span>

<span class="sd">    scale_l2_by : [&quot;group_length&quot;, None], default=&quot;group_length&quot;</span>
<span class="sd">        Scaling technique for the group-wise L2 penalty.</span>
<span class="sd">        By default, ``scale_l2_by=&quot;group_length`` and the L2 penalty is</span>
<span class="sd">        scaled by the square root of the group length so that each variable</span>
<span class="sd">        has the same effect on the penalty. This may not be appropriate for</span>
<span class="sd">        one-hot encoded features and ``scale_l2_by=None`` would be more</span>
<span class="sd">        appropriate for that case. ``scale_l2_by=None`` will also reproduce</span>
<span class="sd">        ElasticNet results when all features belong to one group.</span>

<span class="sd">    eps : float, default=1e-3</span>
<span class="sd">        Length of the path. ``eps=1e-3`` means that</span>
<span class="sd">        ``alpha_min / alpha_max = 1e-3``.</span>

<span class="sd">    n_alphas : int, default=100</span>
<span class="sd">        Number of alphas along the regularization path.</span>

<span class="sd">    alphas : ndarray, default=None</span>
<span class="sd">        List of alphas where to compute the models.</span>
<span class="sd">        If None alphas are set automatically.</span>

<span class="sd">    Xy : array-like of shape (n_features,), default=None</span>
<span class="sd">        Xy = np.dot(X.T, y) that can be precomputed. If supplying ``Xy``,</span>
<span class="sd">        prevent train/test leakage by ensuring the ``Xy`` is precomputed</span>
<span class="sd">        using only training data.</span>

<span class="sd">    normalize : bool, default=False</span>
<span class="sd">        This parameter is ignored when ``fit_intercept`` is set to False.</span>
<span class="sd">        If True, the regressors X will be normalized before regression by</span>
<span class="sd">        subtracting the mean and dividing by the l2-norm.</span>
<span class="sd">        If you wish to standardize, please use</span>
<span class="sd">        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``</span>
<span class="sd">        on an estimator with ``normalize=False``.</span>

<span class="sd">    copy_X : bool, default=True</span>
<span class="sd">        If ``True``, X will be copied; else, it may be overwritten.</span>

<span class="sd">    verbose : bool or int, default=False</span>
<span class="sd">        Amount of verbosity.</span>

<span class="sd">    check_input : bool, default=True</span>
<span class="sd">        Skip input validation checks, assuming there are handled by the</span>
<span class="sd">        caller when check_input=False.</span>

<span class="sd">    **params : kwargs</span>
<span class="sd">        Keyword arguments passed to the SGL estimator</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    coefs : ndarray of shape (n_features, n_alphas) or (n_features + 1, n_alphas)</span>
<span class="sd">        List of coefficients for the Logistic Regression model. If</span>
<span class="sd">        fit_intercept is set to True then the first dimension will be</span>
<span class="sd">        n_features + 1, where the last item represents the intercept.</span>

<span class="sd">    alphas : ndarray of shape (n_alphas,)</span>
<span class="sd">        The alphas along the path where models are computed.</span>

<span class="sd">    n_iters : array of shape (n_alphas,)</span>
<span class="sd">        Actual number of iteration for each alpha.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    SGL</span>
<span class="sd">    SGLCV</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># We expect X and y to be already Fortran ordered when bypassing</span>
    <span class="c1"># checks</span>
    <span class="k">if</span> <span class="n">check_input</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span>
            <span class="n">accept_sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span>
            <span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">,</span>
            <span class="n">copy</span><span class="o">=</span><span class="n">copy_X</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span>
            <span class="n">y</span><span class="p">,</span>
            <span class="n">accept_sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">type</span><span class="p">,</span>
            <span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">,</span>
            <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">ensure_2d</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">Xy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Xy should be a 1d contiguous array</span>
            <span class="n">Xy</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span>
                <span class="n">Xy</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">type</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ensure_2d</span><span class="o">=</span><span class="kc">False</span>
            <span class="p">)</span>
        <span class="n">groups</span> <span class="o">=</span> <span class="n">check_groups</span><span class="p">(</span><span class="n">groups</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">allow_overlap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">_</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="n">fit_intercept</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;fit_intercept&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">alphas</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">alphas</span> <span class="o">=</span> <span class="n">_alpha_grid</span><span class="p">(</span>
            <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>
            <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
            <span class="n">Xy</span><span class="o">=</span><span class="n">Xy</span><span class="p">,</span>
            <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
            <span class="n">scale_l2_by</span><span class="o">=</span><span class="n">scale_l2_by</span><span class="p">,</span>
            <span class="n">l1_ratio</span><span class="o">=</span><span class="n">l1_ratio</span><span class="p">,</span>
            <span class="n">fit_intercept</span><span class="o">=</span><span class="n">fit_intercept</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
            <span class="n">n_alphas</span><span class="o">=</span><span class="n">n_alphas</span><span class="p">,</span>
            <span class="n">normalize</span><span class="o">=</span><span class="n">normalize</span><span class="p">,</span>
            <span class="n">copy_X</span><span class="o">=</span><span class="n">copy_X</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">alphas</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># make sure alphas are properly ordered</span>

    <span class="n">n_alphas</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">alphas</span><span class="p">)</span>
    <span class="n">tol</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;tol&quot;</span><span class="p">,</span> <span class="mf">1e-7</span><span class="p">)</span>
    <span class="n">max_iter</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;max_iter&quot;</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span> <span class="s2">&quot;squared_loss&quot;</span><span class="p">)</span>
    <span class="n">n_iters</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n_alphas</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">fit_intercept</span><span class="p">:</span>
        <span class="n">coefs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n_features</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_alphas</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">coefs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_alphas</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">SGL</span><span class="p">(</span>
        <span class="n">l1_ratio</span><span class="o">=</span><span class="n">l1_ratio</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="n">alphas</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
        <span class="n">scale_l2_by</span><span class="o">=</span><span class="n">scale_l2_by</span><span class="p">,</span>
        <span class="n">fit_intercept</span><span class="o">=</span><span class="n">fit_intercept</span><span class="p">,</span>
        <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
        <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span>
        <span class="n">warm_start</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">suppress_solver_warnings</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">include_solver_trace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">verbose</span> <span class="ow">and</span> <span class="n">verbose</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">alpha_sequence</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Reg path&quot;</span><span class="p">,</span> <span class="n">total</span><span class="o">=</span><span class="n">n_alphas</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">alpha_sequence</span> <span class="o">=</span> <span class="n">alphas</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">alpha_sequence</span><span class="p">):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">fit_intercept</span><span class="p">:</span>
            <span class="n">coefs</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">]])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">coefs</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span>

        <span class="n">n_iters</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">n_iter_</span>

        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Path: </span><span class="si">%03i</span><span class="s2"> out of </span><span class="si">%03i</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">n_alphas</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">coefs</span><span class="p">,</span> <span class="n">alphas</span><span class="p">,</span> <span class="n">n_iters</span>


<span class="k">def</span> <span class="nf">sgl_scoring_path</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span>
    <span class="n">y</span><span class="p">,</span>
    <span class="n">train</span><span class="p">,</span>
    <span class="n">test</span><span class="p">,</span>
    <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">groups</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">scale_l2_by</span><span class="o">=</span><span class="s2">&quot;group_length&quot;</span><span class="p">,</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
    <span class="n">n_alphas</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">Xy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">copy_X</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">check_input</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">scoring</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">params</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return the scores for the models computed by &#39;path&#39;.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        Training data.</span>

<span class="sd">    y : array-like of shape (n_samples,) or (n_samples, n_targets)</span>
<span class="sd">        Target values.</span>

<span class="sd">    train : list of indices</span>
<span class="sd">        The indices of the train set.</span>

<span class="sd">    test : list of indices</span>
<span class="sd">        The indices of the test set.</span>

<span class="sd">    l1_ratio : float, default=0.5</span>
<span class="sd">        Number between 0 and 1 passed to SGL estimator (scaling between the</span>
<span class="sd">        group lasso and lasso penalties). ``l1_ratio=1`` corresponds to the</span>
<span class="sd">        Lasso.</span>

<span class="sd">    groups : list of numpy.ndarray</span>
<span class="sd">        list of arrays of non-overlapping indices for each group. For</span>
<span class="sd">        example, if nine features are grouped into equal contiguous groups of</span>
<span class="sd">        three, then groups would be ``[array([0, 1, 2]), array([3, 4, 5]),</span>
<span class="sd">        array([6, 7, 8])]``. If the feature matrix contains a bias or</span>
<span class="sd">        intercept feature, do not include it as a group. If None, all</span>
<span class="sd">        features will belong to one group.</span>

<span class="sd">    scale_l2_by : [&quot;group_length&quot;, None], default=&quot;group_length&quot;</span>
<span class="sd">        Scaling technique for the group-wise L2 penalty.</span>
<span class="sd">        By default, ``scale_l2_by=&quot;group_length`` and the L2 penalty is</span>
<span class="sd">        scaled by the square root of the group length so that each variable</span>
<span class="sd">        has the same effect on the penalty. This may not be appropriate for</span>
<span class="sd">        one-hot encoded features and ``scale_l2_by=None`` would be more</span>
<span class="sd">        appropriate for that case. ``scale_l2_by=None`` will also reproduce</span>
<span class="sd">        ElasticNet results when all features belong to one group.</span>

<span class="sd">    eps : float, default=1e-3</span>
<span class="sd">        Length of the path. ``eps=1e-3`` means that</span>
<span class="sd">        ``alpha_min / alpha_max = 1e-3``.</span>

<span class="sd">    n_alphas : int, default=100</span>
<span class="sd">        Number of alphas along the regularization path.</span>

<span class="sd">    alphas : ndarray, default=None</span>
<span class="sd">        List of alphas where to compute the models.</span>
<span class="sd">        If None alphas are set automatically.</span>

<span class="sd">    Xy : array-like of shape (n_features,), default=None</span>
<span class="sd">        Xy = np.dot(X.T, y) that can be precomputed. If supplying ``Xy``,</span>
<span class="sd">        prevent train/test leakage by ensuring the ``Xy`` is precomputed</span>
<span class="sd">        using only training data.</span>

<span class="sd">    normalize : bool, default=False</span>
<span class="sd">        This parameter is ignored when ``fit_intercept`` is set to False.</span>
<span class="sd">        If True, the regressors X will be normalized before regression by</span>
<span class="sd">        subtracting the mean and dividing by the l2-norm.</span>
<span class="sd">        If you wish to standardize, please use</span>
<span class="sd">        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``</span>
<span class="sd">        on an estimator with ``normalize=False``.</span>

<span class="sd">    copy_X : bool, default=True</span>
<span class="sd">        If ``True``, X will be copied; else, it may be overwritten.</span>

<span class="sd">    verbose : bool or int, default=False</span>
<span class="sd">        Amount of verbosity.</span>

<span class="sd">    check_input : bool, default=True</span>
<span class="sd">        Skip input validation checks, assuming there are handled by the</span>
<span class="sd">        caller when check_input=False.</span>

<span class="sd">    scoring : callable, default=None</span>
<span class="sd">        A string (see sklearn model evaluation documentation) or a scorer</span>
<span class="sd">        callable object / function with signature ``scorer(estimator, X, y)``.</span>
<span class="sd">        For a list of scoring functions that can be used, look at</span>
<span class="sd">        `sklearn.metrics`. The default scoring option used is accuracy_score.</span>

<span class="sd">    **params : kwargs</span>
<span class="sd">        Keyword arguments passed to the SGL estimator</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    coefs : ndarray of shape (n_features, n_alphas) or (n_features + 1, n_alphas)</span>
<span class="sd">        List of coefficients for the SGL model. If fit_intercept is set to</span>
<span class="sd">        True then the first dimension will be n_features + 1, where the last</span>
<span class="sd">        item represents the intercept.</span>

<span class="sd">    alphas : ndarray</span>
<span class="sd">        Grid of alphas used for cross-validation.</span>

<span class="sd">    scores : ndarray of shape (n_alphas,)</span>
<span class="sd">        Scores obtained for each alpha.</span>

<span class="sd">    n_iter : ndarray of shape(n_alphas,)</span>
<span class="sd">        Actual number of iteration for each alpha.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">Xy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;You supplied the `Xy` parameter. Remember to ensure &quot;</span>
            <span class="s2">&quot;that Xy is computed from the training data alone.&quot;</span>
        <span class="p">)</span>

    <span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">]</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">]</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">test</span><span class="p">]</span>
    <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">]</span>

    <span class="n">coefs</span><span class="p">,</span> <span class="n">alphas</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">=</span> <span class="n">sgl_path</span><span class="p">(</span>
        <span class="n">X_train</span><span class="p">,</span>
        <span class="n">y_train</span><span class="p">,</span>
        <span class="n">l1_ratio</span><span class="o">=</span><span class="n">l1_ratio</span><span class="p">,</span>
        <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
        <span class="n">scale_l2_by</span><span class="o">=</span><span class="n">scale_l2_by</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
        <span class="n">n_alphas</span><span class="o">=</span><span class="n">n_alphas</span><span class="p">,</span>
        <span class="n">alphas</span><span class="o">=</span><span class="n">alphas</span><span class="p">,</span>
        <span class="n">Xy</span><span class="o">=</span><span class="n">Xy</span><span class="p">,</span>
        <span class="n">normalize</span><span class="o">=</span><span class="n">normalize</span><span class="p">,</span>
        <span class="n">copy_X</span><span class="o">=</span><span class="n">copy_X</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
        <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">params</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">del</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span>

    <span class="n">fit_intercept</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;fit_intercept&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">max_iter</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;max_iter&quot;</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
    <span class="n">tol</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;tol&quot;</span><span class="p">,</span> <span class="mf">1e-7</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">SGL</span><span class="p">(</span>
        <span class="n">l1_ratio</span><span class="o">=</span><span class="n">l1_ratio</span><span class="p">,</span>
        <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
        <span class="n">scale_l2_by</span><span class="o">=</span><span class="n">scale_l2_by</span><span class="p">,</span>
        <span class="n">fit_intercept</span><span class="o">=</span><span class="n">fit_intercept</span><span class="p">,</span>
        <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
        <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span>
        <span class="n">warm_start</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">suppress_solver_warnings</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">include_solver_trace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">scoring</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scoring</span> <span class="o">=</span> <span class="s2">&quot;neg_mean_squared_error&quot;</span>

    <span class="n">model</span><span class="o">.</span><span class="n">is_fitted_</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">scores</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="n">scoring</span> <span class="o">=</span> <span class="n">get_scorer</span><span class="p">(</span><span class="n">scoring</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">coefs</span><span class="o">.</span><span class="n">T</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">fit_intercept</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">w</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">w</span>
            <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="mf">0.0</span>

        <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scoring</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">coefs</span><span class="p">,</span> <span class="n">alphas</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="n">n_iter</span>


<div class="viewcode-block" id="SGLCV"><a class="viewcode-back" href="../../api.html#groupyr.SGLCV">[docs]</a><span class="k">class</span> <span class="nc">SGLCV</span><span class="p">(</span><span class="n">LinearModel</span><span class="p">,</span> <span class="n">RegressorMixin</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Iterative SGL model fitting along a regularization path.</span>

<span class="sd">    See the scikit-learn glossary entry for `cross-validation estimator</span>
<span class="sd">    &lt;https://scikit-learn.org/stable/glossary.html#term-cross-validation-estimator&gt;`_</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    l1_ratio : float or list of float, default=1.0</span>
<span class="sd">        float between 0 and 1 passed to SGL (scaling between group lasso and</span>
<span class="sd">        lasso penalties). For ``l1_ratio = 0`` the penalty is the group lasso</span>
<span class="sd">        penalty. For ``l1_ratio = 1`` it is the lasso penalty. For ``0 &lt;</span>
<span class="sd">        l1_ratio &lt; 1``, the penalty is a combination of group lasso and</span>
<span class="sd">        lasso. This parameter can be a list, in which case the different</span>
<span class="sd">        values are tested by cross-validation and the one giving the best</span>
<span class="sd">        prediction score is used. Note that a good choice of list of values</span>
<span class="sd">        will depend on the problem. For problems where we expect strong</span>
<span class="sd">        overall sparsity and would like to encourage grouping, put more</span>
<span class="sd">        values close to 1 (i.e. Lasso). In contrast, if we expect strong</span>
<span class="sd">        group-wise sparsity, but only mild sparsity within groups, put more</span>
<span class="sd">        values close to 0 (i.e. group lasso).</span>

<span class="sd">    groups : list of numpy.ndarray</span>
<span class="sd">        list of arrays of non-overlapping indices for each group. For</span>
<span class="sd">        example, if nine features are grouped into equal contiguous groups of</span>
<span class="sd">        three, then groups would be ``[array([0, 1, 2]), array([3, 4, 5]),</span>
<span class="sd">        array([6, 7, 8])]``. If the feature matrix contains a bias or</span>
<span class="sd">        intercept feature, do not include it as a group. If None, all</span>
<span class="sd">        features will belong to one group. We set groups in ``__init__`` so</span>
<span class="sd">        that it can be reused in model selection and CV routines.</span>

<span class="sd">    scale_l2_by : [&quot;group_length&quot;, None], default=&quot;group_length&quot;</span>
<span class="sd">        Scaling technique for the group-wise L2 penalty.</span>
<span class="sd">        By default, ``scale_l2_by=&quot;group_length`` and the L2 penalty is</span>
<span class="sd">        scaled by the square root of the group length so that each variable</span>
<span class="sd">        has the same effect on the penalty. This may not be appropriate for</span>
<span class="sd">        one-hot encoded features and ``scale_l2_by=None`` would be more</span>
<span class="sd">        appropriate for that case. ``scale_l2_by=None`` will also reproduce</span>
<span class="sd">        ElasticNet results when all features belong to one group.</span>

<span class="sd">    eps : float, default=1e-3</span>
<span class="sd">        Length of the path. ``eps=1e-3`` means that</span>
<span class="sd">        ``alpha_min / alpha_max = 1e-3``.</span>

<span class="sd">    n_alphas : int, default=100</span>
<span class="sd">        Number of alphas along the regularization path, used for each l1_ratio.</span>

<span class="sd">    alphas : ndarray, default=None</span>
<span class="sd">        List of alphas where to compute the models.</span>
<span class="sd">        If None alphas are set automatically</span>

<span class="sd">    fit_intercept : bool, default=True</span>
<span class="sd">        whether to calculate the intercept for this model. If set</span>
<span class="sd">        to false, no intercept will be used in calculations</span>
<span class="sd">        (i.e. data is expected to be centered).</span>

<span class="sd">    normalize : bool, default=False</span>
<span class="sd">        This parameter is ignored when ``fit_intercept`` is set to False.</span>
<span class="sd">        If True, the regressors X will be normalized before regression by</span>
<span class="sd">        subtracting the mean and dividing by the l2-norm.</span>
<span class="sd">        If you wish to standardize, please use</span>
<span class="sd">        :class:`sklearn:sklearn.preprocessing.StandardScaler` before calling ``fit``</span>
<span class="sd">        on an estimator with ``normalize=False``.</span>

<span class="sd">    max_iter : int, default=1000</span>
<span class="sd">        The maximum number of iterations</span>

<span class="sd">    tol : float, default=1e-7</span>
<span class="sd">        Stopping criterion. Convergence tolerance for the ``copt`` proximal gradient solver</span>

<span class="sd">    cv : int, cross-validation generator or iterable, default=None</span>
<span class="sd">        Determines the cross-validation splitting strategy.</span>
<span class="sd">        Possible inputs for cv are:</span>

<span class="sd">        - None, to use the default 5-fold cross-validation,</span>
<span class="sd">        - int, to specify the number of folds.</span>
<span class="sd">        - an sklearn `CV splitter &lt;https://scikit-learn.org/stable/glossary.html#term-cv-splitter&gt;`_,</span>
<span class="sd">        - An iterable yielding (train, test) splits as arrays of indices.</span>

<span class="sd">        For int/None inputs, :class:`sklearn:sklearn.model_selection.KFold` is used.</span>

<span class="sd">        Refer to the :ref:`scikit-learn User Guide</span>
<span class="sd">        &lt;sklearn:cross_validation&gt;` for the various cross-validation</span>
<span class="sd">        strategies that can be used here.</span>

<span class="sd">    copy_X : bool, default=True</span>
<span class="sd">        If ``True``, X will be copied; else, it may be overwritten.</span>

<span class="sd">    verbose : bool or int, default=0</span>
<span class="sd">        Amount of verbosity.</span>

<span class="sd">    n_jobs : int, default=None</span>
<span class="sd">        Number of CPUs to use during the cross validation.</span>
<span class="sd">        ``None`` means 1 unless in a :obj:`joblib:joblib.parallel_backend` context.</span>
<span class="sd">        ``-1`` means using all processors.</span>

<span class="sd">    tuning_strategy : [&quot;grid&quot;, &quot;bayes&quot;], default=&quot;grid&quot;</span>
<span class="sd">        Hyperparameter tuning strategy to use. If ``tuning_strategy == &quot;grid&quot;``,</span>
<span class="sd">        then evaluate all parameter points on the ``l1_ratio`` and ``alphas`` grid,</span>
<span class="sd">        using warm start to evaluate different ``alpha`` values along the</span>
<span class="sd">        regularization path. If ``tuning_strategy == &quot;bayes&quot;``, then a fixed</span>
<span class="sd">        number of parameter settings is sampled using ``skopt.BayesSearchCV``.</span>
<span class="sd">        The fixed number of settings is set by ``n_bayes_iter``. The</span>
<span class="sd">        ``l1_ratio`` setting is sampled uniformly from the minimum and maximum</span>
<span class="sd">        of the input ``l1_ratio`` parameter. The ``alpha`` setting is sampled</span>
<span class="sd">        log-uniformly either from the maximum and minumum of the input ``alphas``</span>
<span class="sd">        parameter, if provided or from ``eps`` * max_alpha to max_alpha where</span>
<span class="sd">        max_alpha is a conservative estimate of the maximum alpha for which the</span>
<span class="sd">        solution coefficients are non-trivial.</span>

<span class="sd">    n_bayes_iter : int, default=50</span>
<span class="sd">        Number of parameter settings that are sampled if using Bayes search</span>
<span class="sd">        for hyperparameter optimization. ``n_bayes_iter`` trades off runtime</span>
<span class="sd">        vs quality of the solution. Consider increasing ``n_bayes_points`` if</span>
<span class="sd">        you want to try more parameter settings in parallel.</span>

<span class="sd">    n_bayes_points : int, default=1</span>
<span class="sd">        Number of parameter settings to sample in parallel if using Bayes</span>
<span class="sd">        search for hyperparameter optimization. If this does not align with</span>
<span class="sd">        ``n_bayes_iter``, the last iteration will sample fewer points.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional (default=None)</span>
<span class="sd">        If int, random_state is the seed used by the random number generator;</span>
<span class="sd">        If RandomState instance, random_state is the random number generator;</span>
<span class="sd">        If None, the random number generator is the RandomState instance used</span>
<span class="sd">        by `np.random`.</span>

<span class="sd">    suppress_solver_warnings : bool, default=True</span>
<span class="sd">        If True, suppress warnings from BayesSearchCV when the objective is</span>
<span class="sd">        evaluated at the same point multiple times. Setting this to False,</span>
<span class="sd">        may be useful for debugging.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    alpha_ : float</span>
<span class="sd">        The amount of penalization chosen by cross validation</span>

<span class="sd">    l1_ratio_ : float</span>
<span class="sd">        The compromise between l1 and l2 penalization chosen by</span>
<span class="sd">        cross validation</span>

<span class="sd">    coef_ : ndarray of shape (n_features,) or (n_targets, n_features)</span>
<span class="sd">        Parameter vector (w in the cost function formula),</span>

<span class="sd">    intercept_ : float or ndarray of shape (n_targets, n_features)</span>
<span class="sd">        Independent term in the decision function.</span>

<span class="sd">    scoring_path_ : ndarray of shape (n_l1_ratio, n_alpha, n_folds)</span>
<span class="sd">        Mean square error for the test set on each fold, varying l1_ratio and</span>
<span class="sd">        alpha.</span>

<span class="sd">    alphas_ : ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)</span>
<span class="sd">        The grid of alphas used for fitting, for each l1_ratio.</span>

<span class="sd">    n_iter_ : int</span>
<span class="sd">        number of iterations run by the proximal gradient descent solver to</span>
<span class="sd">        reach the specified tolerance for the optimal alpha.</span>

<span class="sd">    bayes_optimizer_ : skopt.BayesSearchCV instance or None</span>
<span class="sd">        The BayesSearchCV instance used for hyperparameter optimization if</span>
<span class="sd">        ``tuning_strategy == &quot;bayes&quot;``. If ``tuning_strategy == &quot;grid&quot;``,</span>
<span class="sd">        then this attribute is None.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    sgl_path</span>
<span class="sd">    SGL</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="n">groups</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">scale_l2_by</span><span class="o">=</span><span class="s2">&quot;group_length&quot;</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
        <span class="n">n_alphas</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
        <span class="n">tol</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span>
        <span class="n">copy_X</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">cv</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">tuning_strategy</span><span class="o">=</span><span class="s2">&quot;grid&quot;</span><span class="p">,</span>
        <span class="n">n_bayes_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
        <span class="n">n_bayes_points</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">suppress_solver_warnings</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span> <span class="o">=</span> <span class="n">l1_ratio</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">=</span> <span class="n">groups</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale_l2_by</span> <span class="o">=</span> <span class="n">scale_l2_by</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_alphas</span> <span class="o">=</span> <span class="n">n_alphas</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alphas</span> <span class="o">=</span> <span class="n">alphas</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span> <span class="o">=</span> <span class="n">fit_intercept</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">normalize</span> <span class="o">=</span> <span class="n">normalize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">=</span> <span class="n">max_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">copy_X</span> <span class="o">=</span> <span class="n">copy_X</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cv</span> <span class="o">=</span> <span class="n">cv</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="n">n_jobs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tuning_strategy</span> <span class="o">=</span> <span class="n">tuning_strategy</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_bayes_iter</span> <span class="o">=</span> <span class="n">n_bayes_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_bayes_points</span> <span class="o">=</span> <span class="n">n_bayes_points</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">suppress_solver_warnings</span> <span class="o">=</span> <span class="n">suppress_solver_warnings</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit sparse group lasso linear model.</span>

<span class="sd">        Fit is on grid of alphas and best alpha estimated by cross-validation.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            Training data. Pass directly as Fortran-contiguous data</span>
<span class="sd">            to avoid unnecessary memory duplication. If y is mono-output,</span>
<span class="sd">            X can be sparse.</span>

<span class="sd">        y : array-like of shape (n_samples,) or (n_samples, n_targets)</span>
<span class="sd">            Target values</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tuning_strategy</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;grid&quot;</span><span class="p">,</span> <span class="s2">&quot;bayes&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`tuning_strategy` must be either &#39;grid&#39; or &#39;bayes&#39;; got &quot;</span>
                <span class="s2">&quot;</span><span class="si">{0}</span><span class="s2"> instead.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tuning_strategy</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="c1"># This makes sure that there is no duplication in memory.</span>
        <span class="c1"># Dealing right with copy_X is important in the following:</span>
        <span class="c1"># Multiple functions touch X and subsamples of X and can induce a</span>
        <span class="c1"># lot of duplication of memory</span>
        <span class="n">copy_X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">copy_X</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span>

        <span class="n">check_y_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
            <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="n">ensure_2d</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="ow">or</span> <span class="n">sparse</span><span class="o">.</span><span class="n">isspmatrix</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="c1"># Keep a reference to X</span>
            <span class="n">reference_to_old_X</span> <span class="o">=</span> <span class="n">X</span>
            <span class="c1"># Let us not impose fortran ordering so far: it is</span>
            <span class="c1"># not useful for the cross-validation loop and will be done</span>
            <span class="c1"># by the model fitting itself</span>

            <span class="c1"># Need to validate separately here.</span>
            <span class="c1"># We can&#39;t pass multi_ouput=True because that would allow y to be</span>
            <span class="c1"># csr. We also want to allow y to be 64 or 32 but check_X_y only</span>
            <span class="c1"># allows to convert for 64.</span>
            <span class="n">check_X_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
                <span class="n">accept_sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span>
            <span class="p">)</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validate_separately</span><span class="o">=</span><span class="p">(</span><span class="n">check_X_params</span><span class="p">,</span> <span class="n">check_y_params</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">may_share_memory</span><span class="p">(</span><span class="n">reference_to_old_X</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
                <span class="c1"># X has been copied</span>
                <span class="n">copy_X</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">del</span> <span class="n">reference_to_old_X</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Need to validate separately here.</span>
            <span class="c1"># We can&#39;t pass multi_ouput=True because that would allow y to be</span>
            <span class="c1"># csr. We also want to allow y to be 64 or 32 but check_X_y only</span>
            <span class="c1"># allows to convert for 64.</span>
            <span class="n">check_X_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
                <span class="n">accept_sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span>
                <span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">,</span>
                <span class="n">copy</span><span class="o">=</span><span class="n">copy_X</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validate_separately</span><span class="o">=</span><span class="p">(</span><span class="n">check_X_params</span><span class="p">,</span> <span class="n">check_y_params</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">copy_X</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="n">model</span> <span class="o">=</span> <span class="n">SGL</span><span class="p">()</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">column_or_1d</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">warn</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;X and y have inconsistent dimensions (</span><span class="si">%d</span><span class="s2"> != </span><span class="si">%d</span><span class="s2">)&quot;</span>
                <span class="o">%</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="p">)</span>

        <span class="n">groups</span> <span class="o">=</span> <span class="n">check_groups</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">allow_overlap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># All SGLCV parameters except &quot;cv&quot; and &quot;n_jobs&quot; are acceptable</span>
        <span class="n">path_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
        <span class="n">path_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cv&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">path_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;n_jobs&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">path_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;tuning_strategy&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">path_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;n_bayes_iter&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">path_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;n_bayes_points&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">path_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;random_state&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="n">l1_ratios</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">path_params</span><span class="p">[</span><span class="s2">&quot;l1_ratio&quot;</span><span class="p">])</span>
        <span class="c1"># For the first path, we need to set l1_ratio</span>
        <span class="n">path_params</span><span class="p">[</span><span class="s2">&quot;l1_ratio&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">l1_ratios</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">alphas</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alphas</span>
        <span class="n">n_l1_ratio</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">l1_ratios</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">alphas</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">alphas</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">_alpha_grid</span><span class="p">(</span>
                    <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>
                    <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                    <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
                    <span class="n">scale_l2_by</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">scale_l2_by</span><span class="p">,</span>
                    <span class="n">l1_ratio</span><span class="o">=</span><span class="n">l1_ratio</span><span class="p">,</span>
                    <span class="n">fit_intercept</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">,</span>
                    <span class="n">eps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span>
                    <span class="n">n_alphas</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_alphas</span><span class="p">,</span>
                    <span class="n">normalize</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">normalize</span><span class="p">,</span>
                    <span class="n">copy_X</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">copy_X</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">l1_ratio</span> <span class="ow">in</span> <span class="n">l1_ratios</span>
            <span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Making sure alphas is properly ordered.</span>
            <span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">alphas</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="n">n_l1_ratio</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="c1"># We want n_alphas to be the number of alphas used for each l1_ratio.</span>
        <span class="n">n_alphas</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">alphas</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">path_params</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;n_alphas&quot;</span><span class="p">:</span> <span class="n">n_alphas</span><span class="p">})</span>

        <span class="n">path_params</span><span class="p">[</span><span class="s2">&quot;copy_X&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">copy_X</span>
        <span class="c1"># We are not computing in parallel, we can modify X</span>
        <span class="c1"># inplace in the folds</span>
        <span class="k">if</span> <span class="n">effective_n_jobs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">path_params</span><span class="p">[</span><span class="s2">&quot;copy_X&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>  <span class="c1"># pragma: no cover</span>
            <span class="n">path_params</span><span class="p">[</span><span class="s2">&quot;verbose&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="c1"># init cross-validation generator</span>
        <span class="n">cv</span> <span class="o">=</span> <span class="n">check_cv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cv</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tuning_strategy</span> <span class="o">==</span> <span class="s2">&quot;grid&quot;</span><span class="p">:</span>
            <span class="c1"># Compute path for all folds and compute MSE to get the best alpha</span>
            <span class="n">folds</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
            <span class="n">best_score</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>

            <span class="n">path_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cv&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">path_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;n_jobs&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">path_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;alphas&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">path_params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;l1_ratio&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">path_params</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;groups&quot;</span><span class="p">:</span> <span class="n">groups</span><span class="p">})</span>

            <span class="c1"># We do a double for loop folded in one, in order to be able to</span>
            <span class="c1"># iterate in parallel on l1_ratio and folds</span>
            <span class="n">jobs</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">delayed</span><span class="p">(</span><span class="n">sgl_scoring_path</span><span class="p">)(</span>
                    <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>
                    <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                    <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">,</span>
                    <span class="n">test</span><span class="o">=</span><span class="n">test</span><span class="p">,</span>
                    <span class="n">l1_ratio</span><span class="o">=</span><span class="n">this_l1_ratio</span><span class="p">,</span>
                    <span class="n">alphas</span><span class="o">=</span><span class="n">this_alphas</span><span class="p">,</span>
                    <span class="n">Xy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="o">**</span><span class="n">path_params</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">this_l1_ratio</span><span class="p">,</span> <span class="n">this_alphas</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">l1_ratios</span><span class="p">,</span> <span class="n">alphas</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">folds</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>  <span class="c1"># pragma: no cover</span>
                <span class="n">parallel_verbosity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">-</span> <span class="mi">2</span>
                <span class="k">if</span> <span class="n">parallel_verbosity</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">parallel_verbosity</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">else</span><span class="p">:</span>  <span class="c1"># pragma: no cover</span>
                <span class="n">parallel_verbosity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span>

            <span class="n">score_paths</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span>
                <span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span>
                <span class="n">verbose</span><span class="o">=</span><span class="n">parallel_verbosity</span><span class="p">,</span>
                <span class="o">**</span><span class="n">_joblib_parallel_args</span><span class="p">(</span><span class="n">prefer</span><span class="o">=</span><span class="s2">&quot;threads&quot;</span><span class="p">),</span>
            <span class="p">)(</span><span class="n">jobs</span><span class="p">)</span>

            <span class="n">coefs_paths</span><span class="p">,</span> <span class="n">alphas_paths</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">n_iters</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">score_paths</span><span class="p">)</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="p">(</span><span class="n">n_l1_ratio</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">folds</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">alphas_paths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">alphas_paths</span><span class="p">,</span> <span class="p">(</span><span class="n">n_l1_ratio</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">folds</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">n_iters</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_iters</span><span class="p">,</span> <span class="p">(</span><span class="n">n_l1_ratio</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">folds</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">coefs_paths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="n">coefs_paths</span><span class="p">,</span> <span class="p">(</span><span class="n">n_l1_ratio</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">folds</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_alphas</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="n">mean_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scoring_path_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

            <span class="k">for</span> <span class="n">l1_ratio</span><span class="p">,</span> <span class="n">l1_alphas</span><span class="p">,</span> <span class="n">score_alphas</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">l1_ratios</span><span class="p">,</span> <span class="n">alphas</span><span class="p">,</span> <span class="n">mean_score</span><span class="p">):</span>
                <span class="n">i_best_alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">score_alphas</span><span class="p">)</span>
                <span class="n">this_best_score</span> <span class="o">=</span> <span class="n">score_alphas</span><span class="p">[</span><span class="n">i_best_alpha</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">this_best_score</span> <span class="o">&gt;</span> <span class="n">best_score</span><span class="p">:</span>
                    <span class="n">best_alpha</span> <span class="o">=</span> <span class="n">l1_alphas</span><span class="p">[</span><span class="n">i_best_alpha</span><span class="p">]</span>
                    <span class="n">best_l1_ratio</span> <span class="o">=</span> <span class="n">l1_ratio</span>
                    <span class="n">best_score</span> <span class="o">=</span> <span class="n">this_best_score</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio_</span> <span class="o">=</span> <span class="n">best_l1_ratio</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">alpha_</span> <span class="o">=</span> <span class="n">best_alpha</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">alphas</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">alphas_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">alphas</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">n_l1_ratio</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">alphas_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alphas_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="c1"># Remove duplicate alphas in case alphas is provided.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">alphas_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">alphas</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

            <span class="c1"># Refit the model with the parameters selected</span>
            <span class="n">common_params</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">name</span><span class="p">:</span> <span class="n">value</span>
                <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
            <span class="p">}</span>

            <span class="n">model</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">common_params</span><span class="p">)</span>
            <span class="n">model</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">best_alpha</span>
            <span class="n">model</span><span class="o">.</span><span class="n">l1_ratio</span> <span class="o">=</span> <span class="n">best_l1_ratio</span>
            <span class="n">model</span><span class="o">.</span><span class="n">copy_X</span> <span class="o">=</span> <span class="n">copy_X</span>

            <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">n_iter_</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bayes_optimizer_</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_fitted_</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Set the model with the common input params</span>
            <span class="n">common_params</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">name</span><span class="p">:</span> <span class="n">value</span>
                <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
            <span class="p">}</span>
            <span class="n">model</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">common_params</span><span class="p">)</span>
            <span class="n">model</span><span class="o">.</span><span class="n">copy_X</span> <span class="o">=</span> <span class="n">copy_X</span>

            <span class="n">search_spaces</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">alphas</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">alphas</span><span class="p">),</span> <span class="s2">&quot;log-uniform&quot;</span><span class="p">)}</span>

            <span class="n">l1_min</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span><span class="p">)</span>
            <span class="n">l1_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">l1_min</span> <span class="o">&lt;</span> <span class="n">l1_max</span><span class="p">:</span>
                <span class="n">search_spaces</span><span class="p">[</span><span class="s2">&quot;l1_ratio&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span><span class="p">),</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span><span class="p">),</span>
                    <span class="s2">&quot;uniform&quot;</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">bayes_optimizer_</span> <span class="o">=</span> <span class="n">BayesSearchCV</span><span class="p">(</span>
                <span class="n">model</span><span class="p">,</span>
                <span class="n">search_spaces</span><span class="p">,</span>
                <span class="n">n_iter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_bayes_iter</span><span class="p">,</span>
                <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span>
                <span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span>
                <span class="n">n_points</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_bayes_points</span><span class="p">,</span>
                <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
                <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span>
                <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">scoring</span><span class="o">=</span><span class="s2">&quot;neg_mean_squared_error&quot;</span><span class="p">,</span>
                <span class="n">error_score</span><span class="o">=-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">suppress_solver_warnings</span><span class="p">:</span>
                <span class="n">ctx_mgr</span> <span class="o">=</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">ctx_mgr</span> <span class="o">=</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">suppress</span><span class="p">()</span>

            <span class="k">with</span> <span class="n">ctx_mgr</span><span class="p">:</span>
                <span class="c1"># If n_bayes_points &gt; 1 the objective may be evaluated at the</span>
                <span class="c1"># same point multiple times. This is okay and we give the user</span>
                <span class="c1"># the choice as to whether or not to see these warnings. The</span>
                <span class="c1"># default is to suppress them.</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">suppress_solver_warnings</span><span class="p">:</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">UserWarning</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">bayes_optimizer_</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">alpha_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bayes_optimizer_</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">alpha</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bayes_optimizer_</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">l1_ratio</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bayes_optimizer_</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">coef_</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bayes_optimizer_</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">intercept_</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bayes_optimizer_</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">n_iter_</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_fitted_</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scoring_path_</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">param_alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bayes_optimizer_</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s2">&quot;param__alpha&quot;</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">alphas_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">param_alpha</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">chosen_features_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return index array of chosen features.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">sparsity_mask_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return boolean array indicating which features survived regularization.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">!=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">like_nonzero_mask_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return boolean array indicating which features are zero or close to zero.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        rtol : float</span>
<span class="sd">            Relative tolerance. Any features that are larger in magnitude</span>
<span class="sd">            than ``rtol`` times the mean coefficient value are considered</span>
<span class="sd">            nonzero-like.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">mean_abs_coef</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">rtol</span> <span class="o">*</span> <span class="n">mean_abs_coef</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">chosen_groups_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return set of the group IDs that survived regularization.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">group_mask</span> <span class="o">=</span> <span class="p">[</span>
                <span class="nb">bool</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">grp</span><span class="p">)</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">chosen_features_</span><span class="p">)))</span>
                <span class="k">for</span> <span class="n">grp</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span>
            <span class="p">]</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">group_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">chosen_features_</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Remove columns corresponding to zeroed-out coefficients.&quot;&quot;&quot;</span>
        <span class="c1"># Check is fit had been called</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;is_fitted_&quot;</span><span class="p">)</span>

        <span class="c1"># Input validation</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Check that the input is of the same shape as the one passed</span>
        <span class="c1"># during fit.</span>
        <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Shape of input is different from what was seen in `fit`&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">X</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparsity_mask_</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_more_tags</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># pylint: disable=no-self-use</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;multioutput&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;requires_y&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, Adam Richie-Halford.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>