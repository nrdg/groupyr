{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Specifying a scoring criterion for model selection\n\nEstimates a Sparse Group Lasso logistic regression model on a simulated\nsparse signal with highly imbalanced data. Model hyperparameters are\nchosen through cross-validation. The first model uses default parameters\nand chooses hyperparameters to maximize accuracy, while the second model\nuses F1 score to choose hyperparameters. The model chosen through F1\nscoring shows a modest increase in both F1 score and accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom matplotlib import pyplot as plt\nfrom groupyr import LogisticSGLCV\nfrom groupyr.datasets import make_group_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n\n\nX, y, groups, idx = make_group_classification(\n    n_samples=1000,\n    n_groups=20,\n    n_informative_groups=3,\n    n_features_per_group=20,\n    n_informative_per_group=15,\n    n_redundant_per_group=0,\n    n_repeated_per_group=0,\n    n_classes=2,\n    scale=100,\n    useful_indices=True,\n    random_state=1729,\n    weights=[0.97, 0.03],\n)\n\n# Here we split the data into train and test splits\n# In order to inflate the effect of choosing the F1 score for cross-validation,\n# we neglect to stratify the train/test split.\n# In practice, we would want to provide the ``stratify=y`` parameter.\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\n# Common keyword arguments\nkwargs = dict(groups=groups, l1_ratio=0.5, n_alphas=40, tol=1e-2, cv=3, random_state=0)\n\n# Train a model with default scoring (i.e. accuracy)\ndefault_model = LogisticSGLCV(**kwargs).fit(X_train, y_train)\n\n# And a model with F1 scoring\nf1_model = LogisticSGLCV(scoring=\"f1\", **kwargs).fit(X_train, y_train)\n\n# The model selected using F1 score performs better than the one selected using accuracy.\nheader = \"{model:10s}{metric:20s}{score:5s}\"\nrow = \"{model:10s}{metric:20s}{score:5.3f}\"\n\nprint(header.format(model=\"Model\", metric=\"Metric\", score=\"Score\"))\nprint(\"-\" * len(header.format(model=\"\", metric=\"\", score=\"\")))\nprint(\n    row.format(\n        model=\"default\",\n        metric=\"accuracy\",\n        score=accuracy_score(y_test, default_model.predict(X_test)),\n    )\n)\nprint(\n    row.format(\n        model=\"F1\",\n        metric=\"accuracy\",\n        score=accuracy_score(y_test, f1_model.predict(X_test)),\n    )\n)\nprint()\nprint(\n    row.format(\n        model=\"default\",\n        metric=\"F1 score\",\n        score=f1_score(y_test, default_model.predict(X_test)),\n    )\n)\nprint(\n    row.format(\n        model=\"F1\", metric=\"F1 score\", score=f1_score(y_test, f1_model.predict(X_test))\n    )\n)\nprint()\nprint(\n    row.format(\n        model=\"default\",\n        metric=\"balanced accuracy\",\n        score=balanced_accuracy_score(y_test, default_model.predict(X_test)),\n    )\n)\nprint(\n    row.format(\n        model=\"F1\",\n        metric=\"balanced accuracy\",\n        score=balanced_accuracy_score(y_test, f1_model.predict(X_test)),\n    )\n)\n\n# Plot the classification probabilities for the different models\ndefault_probs = default_model.predict_proba(X_test)[:, 1]\nf1_probs = f1_model.predict_proba(X_test)[:, 1]\njitter = np.random.normal(loc=0.0, scale=0.05, size=f1_probs.shape)\ncolors = plt.get_cmap(\"tab10\").colors\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 8), sharey=True)\n\nfor mask, show_label in zip(\n    [y_test.astype(bool), np.logical_not(y_test)], [True, False]\n):\n    for prediction, ax in zip([default_probs, f1_probs], axes):\n        pred_pos = prediction[mask] > 0.5\n        pred_neg = np.logical_not(pred_pos)\n\n        _ = ax.plot(\n            y_test[mask][pred_pos] + jitter[mask][pred_pos],\n            prediction[mask][pred_pos],\n            \"o\",\n            ms=8,\n            color=colors[0],\n            alpha=0.7,\n            label=\"Predicted positive\" if show_label else None,\n        )\n\n        _ = ax.plot(\n            y_test[mask][pred_neg] + jitter[mask][pred_neg],\n            prediction[mask][pred_neg],\n            \"o\",\n            ms=8,\n            color=colors[1],\n            alpha=0.7,\n            label=\"Predicted negative\" if show_label else None,\n        )\n\nfor ax in axes:\n    _ = ax.set_xticks([0, 1])\n    _ = ax.set_xticklabels([\"True Negative\", \"True Positive\"], fontsize=16)\n    _ = ax.axhline(0.5, ls=\":\", color=\"black\", alpha=0.8)\n    _ = ax.set_xlim(-0.4, 1.4)\n    _ = ax.set_ylim(-0.01, 1.01)\n\n_ = axes[0].set_ylabel(\"Probability of positive prediction\", fontsize=16)\n_ = fig.suptitle(\"Model selection by:\", fontsize=18)\n_ = axes[0].set_title(\"accuracy\", fontsize=18)\n_ = axes[1].set_title(\"F1 score\", fontsize=18)\n_ = axes[1].legend(\n    loc=\"upper center\", bbox_to_anchor=(-0.05, -0.05), ncol=2, fontsize=14\n)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}